{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "def clean_mv(files):\n",
    "    if type(files) == str:\n",
    "        files = [files]\n",
    "    dfs = []\n",
    "    for file_path in files:\n",
    "        # Initialize variables\n",
    "        processed_data = []  # Reset the processed data list\n",
    "        current_date = None  # Reset the current date\n",
    "        lines = []\n",
    "        data_atendimento = anomes = hora = atendimento_codigo = paciente_codigo = paciente_nome = convenio = medico_codigo = medico_nome = centro_de_custos = ''\n",
    "\n",
    "        # Open the file and process each line\n",
    "        with open(f'{file_path}', 'r', encoding='ISO-8859-1') as file:\n",
    "            # headers = next(file)  # Skip the first line of headers\n",
    "            for line in file:\n",
    "                columns = line.strip().split(',')\n",
    "                    \n",
    "                if 'Data de Atendimento:' in line:\n",
    "                    for col in columns[1:]:\n",
    "                        if col:\n",
    "                            data_atendimento = datetime.datetime.strptime(col, \"%d/%m/%Y\").strftime('%Y/%m/%d')\n",
    "                            anomes = datetime.datetime.strptime(col, \"%d/%m/%Y\").strftime('%Y/%m')\n",
    "\n",
    "                if line[1].isdigit():\n",
    "                    if len(columns) == 0:\n",
    "                        pass\n",
    "                    elif len(columns) == 11:\n",
    "                        atendimento_codigo = columns[0]\n",
    "                        paciente_codigo = columns[3]\n",
    "                        paciente_nome = columns[4]\n",
    "                        hora = columns[6]\n",
    "                        convenio = columns[7]\n",
    "                        medico_codigo = columns[8]\n",
    "                        medico_nome = columns[9]\n",
    "                        centro_de_custos = columns[10]\n",
    "                    elif len(columns) == 12:\n",
    "                        atendimento_codigo = columns[0]\n",
    "                        paciente_codigo = columns[2]\n",
    "                        paciente_nome = columns[4]\n",
    "                        hora = columns[6]\n",
    "                        convenio = columns[7]\n",
    "                        medico_codigo = columns[8]\n",
    "                        medico_nome = columns[9]\n",
    "                        centro_de_custos = columns[10]\n",
    "                    elif len(columns) == 13:\n",
    "                        atendimento_codigo = columns[0]\n",
    "                        paciente_codigo = columns[3]\n",
    "                        paciente_nome = columns[5]\n",
    "                        hora = columns[7]\n",
    "                        convenio = columns[8]\n",
    "                        medico_codigo = columns[9]\n",
    "                        medico_nome = columns[10]\n",
    "                        centro_de_custos = columns[11]\n",
    "                    elif len(columns) == 14:\n",
    "                        atendimento_codigo = columns[0]\n",
    "                        paciente_codigo = columns[2]\n",
    "                        paciente_nome = columns[5]\n",
    "                        hora = columns[8]\n",
    "                        convenio = columns[9]\n",
    "                        medico_codigo = columns[10]\n",
    "                        medico_nome = columns[11]\n",
    "                        centro_de_custos = columns[12]\n",
    "\n",
    "                    else:\n",
    "                        print(f'Line not recognized: {len(columns)}, {columns}')\n",
    "                    try:\n",
    "                        # Assuming the structure is consistent with the observed pattern\n",
    "                        processed_data.append({\n",
    "                            'MES': anomes, \n",
    "                            'DIA': f'{data_atendimento} {hora}',\n",
    "                            'ATENDIMENTO_CODIGO': atendimento_codigo,\n",
    "                            'PACIENTE_CODIGO': paciente_codigo,\n",
    "                            'PACIENTE': paciente_nome, \n",
    "                            'CONVENIO': convenio,\n",
    "                            'MEDICO_CODIGO': medico_codigo,\n",
    "                            'MEDICO': medico_nome, \n",
    "                            'CENTRO_CUSTOS': centro_de_custos, \n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(line)\n",
    "                        # Handle the case where there are not enough columns in a row\n",
    "                        print(f\"Row with missing data: {line}\")\n",
    "        df = pd.DataFrame(processed_data)\n",
    "        df.to_csv(f'{file_path}_cleaned.csv', index=False)\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "    print('Obrigado Santo Dr. Fausto por limpar esse arquivo CSV do \"maravilhoso\" MV para mim :) Amém!')\n",
    "\n",
    "    return dfs\n",
    "dfs = clean_mv(['MV Todos prestadores.csv'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install docx -q -U\n",
    "!pip install pip install python-docx -q -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "import unidecode\n",
    "import string\n",
    "\n",
    "formatting_to_status = {\n",
    "    'None 8C98A7': 'PRIMEIRA',\n",
    "    'True 8C98A7': 'FALTANTE',\n",
    "    'None 8FAD70': 'LIVRE',\n",
    "    'None A9B2BD': 'BLOQUEADO',\n",
    "    'None 7C78B2': 'A DEFINIR'\n",
    "}\n",
    "\n",
    "def sys_clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans text by removing any leading/trailing white space, converting it to lowercase, removing\n",
    "    accents, punctuation, and converting to uppercase.\n",
    "    \n",
    "    Args:\n",
    "    text (str): The input text to clean.\n",
    "    \n",
    "    Returns:\n",
    "    str: The cleaned text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert text to string\n",
    "        try:\n",
    "            text = str(text)\n",
    "        except Exception as e:\n",
    "            print(text, 'is not convertible')\n",
    "        # Remove accents, punctuation, and convert to uppercase\n",
    "        text = unidecode.unidecode(text).translate(str.maketrans('', '', string.punctuation)).upper().strip()\n",
    "        # Replace multiple spaces with a single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return text\n",
    "\n",
    "def extract_run_formatting(run):\n",
    "    # Determine strike-through status\n",
    "    strike = 'True' if run.font.strike else 'None'\n",
    "    # Get the color of the text\n",
    "    color = run.font.color.rgb if run.font.color else 'None'\n",
    "    # Combine the formatting into a string\n",
    "    formatting = f'{strike} {color}'\n",
    "    # Apply the mapping to convert formatting to status\n",
    "    status = formatting_to_status.get(formatting, 'UNKNOWN')\n",
    "    \n",
    "    return status\n",
    "\n",
    "# Função para verificar se uma string está no formato de data dd/mm\n",
    "def parse_date(string):\n",
    "    try:\n",
    "        # Tentar converter a string para um objeto datetime\n",
    "        return datetime.strptime(string, '%d/%m/%Y')\n",
    "    except ValueError:\n",
    "        # Retornar None se a string não for uma data válida\n",
    "        return None\n",
    "\n",
    "# Função para processar o documento e criar o dataframe\n",
    "def create_dataframe_from_docx(paragraphs):\n",
    "    data = []\n",
    "    current_date = None\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        text = paragraph.text.strip()\n",
    "        # Attempt to convert text to date\n",
    "        date_attempt = parse_date(text)\n",
    "        if date_attempt:\n",
    "            current_date = date_attempt  # Update current date if it's a valid date\n",
    "        else:\n",
    "            match = re.search(r'(\\d{2}:\\d{2}) (.+?) 1ª CONSULTA', text)\n",
    "            if match and current_date:\n",
    "                time = match.group(1)\n",
    "                patient_name = match.group(2).strip()\n",
    "                formatting = [extract_run_formatting(run) for run in paragraph.runs if patient_name in run.text][0]\n",
    "                # Format the date and time into MES and DIA format\n",
    "                mes = current_date.strftime('%Y/%m')\n",
    "                dia = f\"{current_date.strftime('%Y/%m/%d')} {time}\"\n",
    "                # Clean the patient name\n",
    "                paciente_cleaned = sys_clean_text(patient_name)\n",
    "                data.append([mes, dia, paciente_cleaned, formatting])\n",
    "    \n",
    "    # Create dataframe with new column names\n",
    "    df = pd.DataFrame(data, columns=['MES', 'DIA', 'PACIENTE', 'GERCON'])\n",
    "    return df\n",
    "\n",
    "# Carregar o documento docx\n",
    "doc = Document('GERCON 12 meses.docx')\n",
    "\n",
    "# Criar o dataframe a partir do documento\n",
    "df_gercon = create_dataframe_from_docx(doc.paragraphs)\n",
    "df_gercon.to_csv('df_gercon.csv', index=False)\n",
    "df_gercon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GERCON'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = pd.merge(df_gercon, df_crv, on='PACIENTE', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_date_adjusted(date_str):\n",
    "    if pd.isnull(date_str):\n",
    "        return None\n",
    "    parts = date_str.split('/')\n",
    "    # Se a data está incompleta ou em um formato diferente, ajusta para o formato correto\n",
    "    if len(parts) == 2:  # Se apenas dia e mês são fornecidos, assumimos o ano atual (2023).\n",
    "        parts.append('2023')\n",
    "    if len(parts[0]) == 4:  # Se a data está no formato YYYY/MM/DD, reorganize para DD/MM/YYYY\n",
    "        return f\"{parts[2]}/{parts[1]}/{parts[0]}\"\n",
    "    # Adiciona zero ao dia ou mês se necessário\n",
    "    day = parts[0].zfill(2)\n",
    "    month = parts[1].zfill(2)\n",
    "    # Corrige o ano, se estiver em dois dígitos ou incorreto\n",
    "    year = parts[2] if len(parts[2]) == 4 else f\"20{parts[2]}\"\n",
    "    # Corrige anos conhecidos que estão incorretos\n",
    "    if year in ['2028', '2003', '1980']:\n",
    "        year = '2023'\n",
    "    return f\"{day}/{month}/{year}\"\n",
    "\n",
    "df_recursos_opticos = pd.read_csv('recursos_opticos.csv')\n",
    "df_recursos_opticos['VALOR'] = df_recursos_opticos['VALOR'].str.replace(',', '.').astype(float)\n",
    "\n",
    "# Aplicar a função de padronização ajustada às colunas 'TESTE' e 'PEDIDO'\n",
    "df_recursos_opticos['TESTE'] = df_recursos_opticos['TESTE'].apply(standardize_date_adjusted)\n",
    "df_recursos_opticos['PEDIDO'] = df_recursos_opticos['PEDIDO'].apply(standardize_date_adjusted)\n",
    "\n",
    "# Tentar converter as colunas 'TESTE' e 'PEDIDO' para datetime novamente\n",
    "df_recursos_opticos['TESTE'] = pd.to_datetime(df_recursos_opticos['TESTE'], format='%d/%m/%Y', errors='coerce')\n",
    "df_recursos_opticos['PEDIDO'] = pd.to_datetime(df_recursos_opticos['PEDIDO'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "# Sanitizar os nomes de paciente\n",
    "df_recursos_opticos['PACIENTE'] = df_recursos_opticos['PACIENTE'].apply(sys_clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARISTEU SOARES NETO -> TAIS PATRICIA SOARES DA SILVA? 86%\n",
      "SETRO CRV -> PEDRO CARVALHO SIQUEIRA? 70%\n",
      "FRANCIELE CRISTINA RODRIGUES DOS SANTOS -> DANIEL RODRIGUES FLORES? 86%\n",
      "ALESSANDRO PINHEIRO MATTOS -> ALESSANDRO PINHEIRO DE MAT? 88%\n",
      "ARIEL LACERDA KONSKY -> ARIEL DE LACERDA KOLESNY? 82%\n",
      "SETOR CRV -> PEDRO CARVALHO SIQUEIRA? 60%\n",
      "CARMEN REGINA RODRIGUES ISRAEL -> CARMEM REGINA RODRIGUEZ IS? 86%\n",
      "ALICE DA SILVA MENEZES -> LACI DA SILVA? 86%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ALESSANDRO PINHEIRO MATTOS',\n",
       " 'ALICE DA SILVA MENEZES',\n",
       " 'ARIEL LACERDA KONSKY',\n",
       " 'ARISTEU SOARES NETO',\n",
       " 'CARMEN REGINA RODRIGUES ISRAEL',\n",
       " 'FRANCIELE CRISTINA RODRIGUES DOS SANTOS',\n",
       " 'SETOR CRV',\n",
       " 'SETRO CRV'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fuzzywuzzy import process\n",
    "import pandas as pd\n",
    "\n",
    "df_mv = pd.read_csv('df_mv.csv')\n",
    "\n",
    "def correct_name(wrong_names, correct_names, threshold=90):\n",
    "    name_corrections = {}\n",
    "    \n",
    "    for wrong_name in wrong_names:\n",
    "        match, similarity = process.extractOne(wrong_name, correct_names)\n",
    "        \n",
    "        if similarity >= threshold:\n",
    "            name_corrections[wrong_name] = match\n",
    "            # print(f'{wrong_name} -> {match} {similarity}%')\n",
    "        else: \n",
    "            print(f'{wrong_name} -> {match}? {similarity}%')\n",
    "    \n",
    "    return name_corrections\n",
    "\n",
    "manual_name_corrections = {\n",
    "    'ARIEL LACERDA KONSKY': 'ARIEL DE LACERDA KOLESNY',\n",
    "    'ALESSANDRO PINHEIRO MATTOS': 'ALESSANDRO PINHEIRO DE MAT',\n",
    "    'CARMEN REGINA RODRIGUES ISRAEL': 'CARMEN REGINA DE OLVEIRA G', \n",
    "    'ARISTEU SOARES NETO': 'ARISTEU SOARES CASTRO', \n",
    "    'ALICE DA SILVA MENEZES': 'ALICE KREFTA DA SILVA MENEZE', \n",
    "    # 'FRANCIELE CRISTINA RODRIGUES DOS SANTOS': 'FRANCIELE CRISTINA RODRIGUE', \n",
    "    # adicione todos os outros nomes que precisam ser corrigidos\n",
    "}\n",
    "df_recursos_opticos['PACIENTE'] = df_recursos_opticos['PACIENTE'].apply(lambda x: manual_name_corrections.get(x, x))\n",
    "\n",
    "# Converting names to lowercase for comparison to avoid case sensitivity issues\n",
    "unique_names_mv = set(df_mv['PACIENTE'].unique())\n",
    "unique_names_recursos_opticos = set(df_recursos_opticos['PACIENTE'].unique())\n",
    "\n",
    "mv_only = unique_names_mv - unique_names_recursos_opticos\n",
    "recursos_opticos_only = unique_names_recursos_opticos - unique_names_mv\n",
    "\n",
    "# Obtaining the correction dictionary\n",
    "name_corrections_dict = correct_name(recursos_opticos_only, unique_names_mv)\n",
    "\n",
    "# Applying the name corrections to df_ro\n",
    "df_recursos_opticos['PACIENTE'] = df_recursos_opticos['PACIENTE'].apply(lambda x: name_corrections_dict.get(x, x))\n",
    "\n",
    "# Atualizar os conjuntos de nomes após a aplicação das correções\n",
    "updated_unique_names_ro = set(df_recursos_opticos['PACIENTE'].unique())\n",
    "\n",
    "# Encontrar nomes que ainda estão apenas em recursos ópticos e não em mv (ou vice-versa)\n",
    "remaining_ro_only = updated_unique_names_ro - unique_names_mv\n",
    "remaining_mv_only = unique_names_mv - updated_unique_names_ro\n",
    "remaining_ro_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recursos_opticos.to_csv('df_recursos_opticos_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OCULOS ASFERICOS': {'quantity': 1, 'unit_price': 645.0},\n",
       " 'OCULOS PRISMATICOS ARO PQ 10/12': {'quantity': 4, 'unit_price': 645.0},\n",
       " 'OCULOS PRISMATICOS ARO PQ 12/14': {'quantity': 4, 'unit_price': 645.0},\n",
       " 'OCULOS PRISMATICO ARO PQ 4/6': {'quantity': 3, 'unit_price': 645.0},\n",
       " 'OCULOS PRISMATICOS ARO PQ 6/8': {'quantity': 5, 'unit_price': 645.0},\n",
       " 'OCULOS PRISMATICOS ARO PQ 8/10': {'quantity': 12, 'unit_price': 645.0},\n",
       " '4X12 TC MONOCULAR': {'quantity': 12, 'unit_price': 528.0},\n",
       " 'LUPA DE APOIO 80MM AS 3X LED BAT.': {'quantity': 1, 'unit_price': 655.0},\n",
       " 'LUPA DE MAO 24D 6X 58MM LED': {'quantity': 10, 'unit_price': 495.0},\n",
       " 'LUPA DE MAO 35MM 7X LED': {'quantity': 6, 'unit_price': 427.0},\n",
       " 'LUPA MAX TV': {'quantity': 2, 'unit_price': 573.0},\n",
       " 'LUPA PESO DE PAPEL 2X': {'quantity': 1, 'unit_price': 519.0},\n",
       " 'LUPA DE APOIO MEDIA 4X 64 MM': {'quantity': 1, 'unit_price': 350.0}}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary for supplier's price list with items as keys and prices as values\n",
    "supplier_prices = {\n",
    "    \"OCULOS ASFERICOS\": 645.00,  # Assumed common price for different dioptrias\n",
    "    \"OCULOS PRISMATICOS ARO PQ 10/12\": 645.00,\n",
    "    \"OCULOS PRISMATICOS ARO PQ 12/14\": 645.00,\n",
    "    \"OCULOS PRISMATICO ARO PQ 4/6\": 645.00,\n",
    "    \"OCULOS PRISMATICOS ARO PQ 6/8\": 645.00,\n",
    "    \"OCULOS PRISMATICOS ARO PQ 8/10\": 645.00,\n",
    "    \"4X12 TC MONOCULAR\": 528.00,\n",
    "    \"LUPA DE APOIO 80MM AS 3X LED BAT.\": 655.00,\n",
    "    \"LUPA DE MAO 24D 6X 58MM LED\": 495.00,\n",
    "    \"LUPA DE MAO 35MM 7X LED\": 427.00,\n",
    "    \"LUPA MAX TV\": 573.00,\n",
    "    \"LUPA PESO DE PAPEL 2X\": 519.00,  # Assuming the closest item \"LUPA TIPO PEDRA 62 cm 4x\" price\n",
    "    \"LUPA DE APOIO MEDIA 4X 64 MM\": 350.00  # Assuming the closest item \"LUPA DE APOIO 80MM AS 3X LED BAT.\" price\n",
    "}\n",
    "\n",
    "# Dictionary for items and quantities from invoice 13.778\n",
    "invoice_13778_items = {\n",
    "    \"OCULOS ASFERICOS\": 1,\n",
    "    \"OCULOS PRISMATICOS ARO PQ 10/12\": 4,\n",
    "    \"OCULOS PRISMATICOS ARO PQ 12/14\": 4,\n",
    "    \"OCULOS PRISMATICO ARO PQ 4/6\": 3,\n",
    "    \"OCULOS PRISMATICOS ARO PQ 6/8\": 5,\n",
    "    \"OCULOS PRISMATICOS ARO PQ 8/10\": 12,\n",
    "    \"4X12 TC MONOCULAR\": 12\n",
    "}\n",
    "\n",
    "# Dictionary for items and quantities from invoice 13.714\n",
    "invoice_13714_items = {\n",
    "    \"LUPA DE APOIO 80MM AS 3X LED BAT.\": 1,\n",
    "    \"LUPA DE MAO 24D 6X 58MM LED\": 10,\n",
    "    \"LUPA DE MAO 35MM 7X LED\": 6,\n",
    "    \"LUPA MAX TV\": 2,\n",
    "    \"LUPA PESO DE PAPEL 2X\": 1,\n",
    "    \"LUPA DE APOIO MEDIA 4X 64 MM\": 1\n",
    "}\n",
    "\n",
    "# Combine the invoice items and quantities with the supplier prices\n",
    "# We will create a new dictionary to store items with their common quantities and prices\n",
    "combined_items = {}\n",
    "\n",
    "# Combine items from invoice 13778\n",
    "for item, qty in invoice_13778_items.items():\n",
    "    if item in supplier_prices:\n",
    "        combined_items[item] = {\n",
    "            \"quantity\": qty,\n",
    "            \"unit_price\": supplier_prices[item]\n",
    "        }\n",
    "\n",
    "# Combine items from invoice 13714\n",
    "for item, qty in invoice_13714_items.items():\n",
    "    if item in supplier_prices:\n",
    "        # If the item is already in the combined_items, we add the quantities\n",
    "        if item in combined_items:\n",
    "            combined_items[item][\"quantity\"] += qty\n",
    "        else:\n",
    "            combined_items[item] = {\n",
    "                \"quantity\": qty,\n",
    "                \"unit_price\": supplier_prices[item]\n",
    "            }\n",
    "\n",
    "combined_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "\n",
    "# Open the PDF file\n",
    "folder = r'C:\\Users\\faust\\Downloads'\n",
    "pdf_path = folder + '\\' + r'LISTA DE PREÇOS ATUAL 2023 - BAIXA VISÃO - SOMENTE REVENDA.pdf'\n",
    "pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "# Read the text of each page\n",
    "text_content = ''\n",
    "for page_number in range(len(pdf_document)):\n",
    "    page = pdf_document.load_page(page_number)\n",
    "    text_content += page.get_text()\n",
    "\n",
    "# Close the PDF after extraction\n",
    "pdf_document.close()\n",
    "\n",
    "# Save the text to a .txt file\n",
    "txt_path = '/mnt/data/extracted_text.txt'\n",
    "with open(txt_path, 'w') as txt_file:\n",
    "    txt_file.write(text_content)\n",
    "\n",
    "# Provide the path to the extracted text file\n",
    "txt_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
