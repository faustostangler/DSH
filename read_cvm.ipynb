{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Defs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import zipfile\n",
    "import os\n",
    "import io\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import unidecode\n",
    "import string\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'dfp'\n",
    "radical = 'cia_aberta'\n",
    "extension = '.zip'\n",
    "demo_cvmnstracoes_financeiras = ['DRA', 'DMPL', 'DFC_MD', 'DFC_MI', 'BPA', 'BPP', 'DRE', 'DVA']  # Add all other items\n",
    "demo_cvmnstracoes_financeiras_dict = {\n",
    "    'BPA': 'Balanço Patrimonial Ativo (BPA)',\n",
    "    'BPP': 'Balanço Patrimonial Passivo (BPP)',\n",
    "    'DFC_MD': 'demo_cvmnstração de Fluxo de Caixa - Método Direto (DFC-MD)',\n",
    "    'DFC_MI': 'demo_cvmnstração de Fluxo de Caixa - Método Indireto (DFC-MI)',\n",
    "    'DMPL': 'demo_cvmnstração de Mutações do Patrimônio Líquido (DMPL)',\n",
    "    'DRA': 'demo_cvmnstração de Resultado Abrangente (DRA)',\n",
    "    'DRE': 'demo_cvmnstração de Resultado (DRE)',\n",
    "    'DVA': 'demo_cvmnstração de Valor Adicionado (DVA)'\n",
    "}\n",
    "base_de_consolidacao = ['ind', 'con']  # Add other variables if existing\n",
    "base_de_consolidacao_dict = {\n",
    "    'ind': 'Individual',\n",
    "    'con': 'Consolidado'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL base do site\n",
    "base_cvm = \"https://dados.cvm.gov.br/dados/CIA_ABERTA/\"\n",
    "\n",
    "# Inicializar uma sessão\n",
    "session = requests.Session()\n",
    "\n",
    "# Lista para armazenar links de arquivos CSV e ZIP\n",
    "files_list = []\n",
    "\n",
    "# Conjunto para armazenar subpastas já visitadas\n",
    "visited_subfolders = set()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_category = ['FILE_NAME', 'demonstrativo_cvm', 'BALANCE_SHEET', 'ANO', 'AGRUPAMENTO', 'CNPJ_CIA', 'VERSAO', 'DENOM_CIA', 'CD_CVM', 'GRUPO_DFP', 'MOEDA', 'ESCALA_MOEDA', 'ORDEM_EXERC', 'CD_CONTA', 'DS_CONTA', 'ST_CONTA_FIXA', 'COLUNA_DF']\n",
    "col_datetime = ['DT_REFER', 'DT_FIM_EXERC', 'DT_INI_EXERC']\n",
    "col_float = ['VL_CONTA']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load pkl\n",
    "def save_pkl(demo_cvm, file_name):\n",
    "    # Save dump pickle\n",
    "    with open(f'{file_name}.pkl', 'wb') as f:\n",
    "        pickle.dump(demo_cvm, f)\n",
    "    return demo_cvm\n",
    "\n",
    "def load_pkl(file_name):\n",
    "    # Read load pickle\n",
    "    with open(f'{file_name}.pkl', 'rb') as f:\n",
    "        demo_cvm = pickle.load(f)\n",
    "    return demo_cvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remaining_time(start_time, size, i):\n",
    "    # Calculate the number of items processed and the number of remaining items\n",
    "    counter = i + 1\n",
    "    remaining_items = size - counter\n",
    "    \n",
    "    # Calculate the percentage of completion\n",
    "    percentage = counter / size\n",
    "    \n",
    "    # Calculate the elapsed time\n",
    "    running_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate the average time taken per item\n",
    "    avg_time_per_item = running_time / counter\n",
    "    \n",
    "    # Calculate the remaining time based on the average time per item\n",
    "    remaining_time = remaining_items * avg_time_per_item\n",
    "    \n",
    "    # Convert remaining time to hours, minutes, and seconds\n",
    "    hours, remainder = divmod(int(remaining_time), 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    \n",
    "    # Format remaining time as a string\n",
    "    remaining_time_formatted = f'{int(hours)}h {int(minutes):02}m {int(seconds):02}s'\n",
    "    \n",
    "    # Create a progress string with all the calculated values\n",
    "    progress = (\n",
    "        f'{percentage:.2%} '\n",
    "        f'{counter}+{remaining_items}, '\n",
    "        f'{avg_time_per_item:.6f}s per item, '\n",
    "        f'Remaining: {remaining_time_formatted}'\n",
    "    )\n",
    "\n",
    "    return progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função auxiliar para reunir links\n",
    "def gather_links(url):\n",
    "    visited_subfolders.add(url)  # Marcar a subpasta como visitada\n",
    "    response = session.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        href = link.get(\"href\")\n",
    "        full_link = urljoin(url, href)\n",
    "\n",
    "        if full_link.startswith(base_cvm) and full_link not in visited_subfolders:\n",
    "            if full_link.endswith((\".csv\", \".zip\", \".txt\")):\n",
    "                files_list.append(full_link)\n",
    "            elif full_link.endswith(\"/\"):\n",
    "                gather_links(full_link)\n",
    "    return files_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper for categories\n",
    "def get_categories(files_list):\n",
    "    # Extrair categorias e arquivos meta\n",
    "    categories = set()\n",
    "    meta_files = [file_link for file_link in files_list if \"meta\" in file_link]\n",
    "    files = [file_link for file_link in files_list if \"meta\" not in file_link]\n",
    "\n",
    "    for file_link in files_list:\n",
    "        cat = '/'.join(file_link.replace(base_cvm,'').split('/')[:-2])\n",
    "        categories.add(cat)\n",
    "    categories = sorted(list(categories))\n",
    "\n",
    "    return categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função auxiliar para extrair metadados\n",
    "def extract_meta(content):\n",
    "    meta_dict = {}\n",
    "    blocks = content.split(\"-----------------------\\r\\nCampo: \")[1:]\n",
    "    \n",
    "    for block in blocks:\n",
    "        lines = block.strip().split(\"\\r\\n\")\n",
    "        campo = lines[0]\n",
    "        descricao = None\n",
    "        \n",
    "        for line in lines:\n",
    "            if 'Descrição' in line or 'Descrio' in line:\n",
    "                descricao = line.split(':')[1].strip()\n",
    "                break\n",
    "        \n",
    "        if descricao is not None:\n",
    "            meta_dict[campo] = descricao\n",
    "    \n",
    "    return meta_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrair e processar metadados\n",
    "def get_metadados(files_list):\n",
    "    meta_dict = {}\n",
    "    meta_files = [file_link for file_link in files_list if \"meta\" in file_link]\n",
    "\n",
    "    for file in meta_files:\n",
    "        response = session.get(file)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        if file.endswith('.zip'):\n",
    "            zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "            for file_in_zip in zip_file.namelist():\n",
    "                with zip_file.open(file_in_zip) as zip_file_content:\n",
    "                    file_content = zip_file_content.read().decode('utf-8', errors='ignore')\n",
    "                    d = extract_meta(file_content)\n",
    "                    meta_dict[file_in_zip.split('.')[0]] = d\n",
    "        elif file.endswith('.txt'):\n",
    "            file_content = response.content.decode('iso-8859-1')\n",
    "            d = extract_meta(file_content)\n",
    "            file_name = file.split('/')[-1].split('.')[0]\n",
    "            meta_dict[file_name] = d\n",
    "\n",
    "    return meta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract financial sheets from web zip files\n",
    "def download_database(demonstrativos_cvm):\n",
    "    # Initialize variables\n",
    "    fin_sheet = []\n",
    "    fin_sheet_links = []\n",
    "    total_size = 0  \n",
    "    total_size_csv = 0\n",
    "    total_rows = 0\n",
    "    dataframes = []\n",
    "\n",
    "    for i, demonstrativo_cvm in enumerate(demonstrativos_cvm):\n",
    "        print(remaining_time(start_time, len(demonstrativos_cvm), i))\n",
    "        # Retrieve the list of files based on the specified 'demonstrativo_cvm'\n",
    "        download_files = [file_link for file_link in files_list if 'meta' not in file_link and demonstrativo_cvm in file_link]\n",
    "\n",
    "        # Iterate through the list of URLs\n",
    "        start_time_2 = time.time()\n",
    "        for j, zip_url in enumerate(download_files):\n",
    "            print('  ' + remaining_time(start_time_2, len(download_files), j))\n",
    "            # Download the zip file\n",
    "            response = requests.get(zip_url)\n",
    "            \n",
    "            # Check if the download was successful\n",
    "            if response.status_code == 200:\n",
    "                # Get the size of the downloaded file\n",
    "                file_size = len(response.content)/(1024 ** 2)\n",
    "                total_size += file_size\n",
    "                print(f'{zip_url}, {file_size:.3f} Mb, {total_size:.3f} Mb total')\n",
    "\n",
    "                # Extract the zip file in memory\n",
    "                with zipfile.ZipFile(BytesIO(response.content)) as zip_ref:\n",
    "                    # Iterate through the files in the zip\n",
    "                    start_time_3 = time.time()\n",
    "                    for k, file_info in enumerate(zip_ref.infolist()):\n",
    "                        # print('  ' + '  ' + remaining_time(start_time_3, len(zip_ref.infolist()), k))\n",
    "                        # Check if the file is a CSV\n",
    "                        if file_info.filename.lower().endswith('.csv'):\n",
    "                            # file size\n",
    "                            csv_size = file_info.file_size/(1024 ** 2)\n",
    "                            total_size_csv += csv_size/(1024 ** 1)\n",
    "\n",
    "                            # Extract the CSV file\n",
    "                            csv_content = zip_ref.read(file_info.filename)\n",
    "                            csv_filename = os.path.basename(file_info.filename)\n",
    "\n",
    "                            # Extract metadata from the CSV filename\n",
    "                            meta_csv = csv_filename.replace('cia_aberta_', '').replace('.csv', '').split('_')\n",
    "                            ano = meta_csv[-1]\n",
    "                            demonstrativo_cvm = meta_csv[0]\n",
    "                            meta_csv = meta_csv[1:-1]\n",
    "                            if len(meta_csv) > 0:\n",
    "                                agrupamento = meta_csv[-1]\n",
    "                                meta_csv = meta_csv[:-1]\n",
    "                            else:\n",
    "                                agrupamento = ''\n",
    "                            balance_sheet = '_'.join(meta_csv)\n",
    "\n",
    "                            # Read CSV content into a pandas DataFrame\n",
    "                            csv_data = pd.read_csv(BytesIO(csv_content), encoding='iso-8859-1', sep=';')\n",
    "\n",
    "                            # Add metadata columns to the DataFrame\n",
    "                            csv_data.insert(0, 'FILE_NAME', csv_filename)\n",
    "                            csv_data.insert(1, 'demonstrativo_cvm', demonstrativo_cvm)\n",
    "                            csv_data.insert(2, 'BALANCE_SHEET', balance_sheet)\n",
    "                            csv_data.insert(3, 'ANO', ano)\n",
    "                            csv_data.insert(4, 'AGRUPAMENTO', agrupamento)\n",
    "\n",
    "                            # Append the DataFrame to the list\n",
    "                            dataframes.append(csv_data)\n",
    "                            total_rows += len(csv_data)\n",
    "                            # print(f'{file_info.filename} {csv_size:.3f} Mb, {total_size_csv:.3f} Gb total, {total_rows:,.0f} total rows')\n",
    "    print(f'Total {len(dataframes)} databases found and {total_rows} downloaded')\n",
    "\n",
    "\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Check if the value is a string\n",
    "        cleaned_text = unidecode.unidecode(text).translate(str.maketrans('', '', string.punctuation)).upper().strip()\n",
    "        return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words from cell\n",
    "def clean_cell(cell):\n",
    "    words_to_remove = ['  EM LIQUIDACAO', ' EM LIQUIDACAO', ' EXTRAJUDICIAL', '  EM RECUPERACAO JUDICIAL', '  EM REC JUDICIAL', ' EM RECUPERACAO JUDICIAL']\n",
    "    for word in words_to_remove:\n",
    "        if word in cell:\n",
    "            cell = cell.replace(word, '').strip()\n",
    "    return cell\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_DT_INI_EXERC(demo_cvm):\n",
    "    print('double clean dataframes')\n",
    "    try:\n",
    "        lines_removed = 0\n",
    "        for i, (year, df) in enumerate(demo_cvm.items()):\n",
    "            size = len(df)\n",
    "            \n",
    "            # Apply the condition to filter the DataFrame\n",
    "            mask = (df['DT_INI_EXERC'] == pd.to_datetime(str(year) + '-01-01')) | df['DT_INI_EXERC'].isna()\n",
    "            df_filtered = df[mask].copy()  # Make a copy to avoid modifying the original DataFrame\n",
    "            \n",
    "            # Update the 'MATH_MAGIC' column for the filtered rows using .loc indexer\n",
    "            df_filtered.loc[:, 'MATH_MAGIC'] = False\n",
    "            \n",
    "            # Update the dictionary with the filtered DataFrame\n",
    "            demo_cvm[year] = df_filtered\n",
    "            \n",
    "            lines_removed += size - len(df_filtered)\n",
    "            print(year, remaining_time(start_time, len(demo_cvm), i))\n",
    "        print(f'{lines_removed} lines removed')\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    return demo_cvm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean dataframes\n",
    "def clean_dataframes(dict_of_df):\n",
    "    print('clean dataframes')\n",
    "    col_datetime = ['DT_REFER', 'DT_FIM_EXERC', 'DT_INI_EXERC']\n",
    "\n",
    "    for i, (year, df) in enumerate(dict_of_df.items()):\n",
    "        print(year, remaining_time(start_time, len(dict_of_df), i))\n",
    "        # remove extra rows\n",
    "        try:\n",
    "            df = df[df['ORDEM_EXERC'] == 'ÚLTIMO']\n",
    "            df = df.drop(columns=['ORDEM_EXERC'])\n",
    "        except Exception as e:\n",
    "            # print(e)\n",
    "            pass\n",
    "\n",
    "        # # remove extra rows\n",
    "        # try:\n",
    "        #     df = df[(df['DT_INI_EXERC'] == pd.to_datetime(str(year) + '-01-01')) | df['DT_INI_EXERC'].isna()]\n",
    "        # except Exception as e:\n",
    "        #     pass\n",
    "        \n",
    "        # Clean up Text\n",
    "        try:\n",
    "            df['DENOM_CIA'] = df['DENOM_CIA'].apply(clean_text)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "        # to datetime\n",
    "        try:\n",
    "            df[col_datetime] = df[col_datetime].apply(pd.to_datetime)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "        # remove specific words\n",
    "        df['DENOM_CIA'] = df['DENOM_CIA'].apply(clean_cell)\n",
    "\n",
    "        dict_of_df[year] = df\n",
    "\n",
    "    return dict_of_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make yearly dict_of_df\n",
    "def yearly(df_list):\n",
    "    df_y = {}\n",
    "    print('group by year')\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate through each DataFrame in the 'demo_cvm' list\n",
    "    for i, df in enumerate(df_list):\n",
    "        # Get the year from the 'ANO' column\n",
    "        # year = int(df['ANO'].iloc[0])  # Assuming the 'ANO' value is the same for all rows in a DataFrame\n",
    "        year = pd.to_datetime(df['DT_REFER']).dt.year.iloc[0]  # Extracting the year from the date\n",
    "        print(year, remaining_time(start_time, len(df_list), i))\n",
    "\n",
    "        # Check if the year is already a key in the dictionary, if not, create a list for it\n",
    "        if year not in df_y:\n",
    "            df_y[year] = []\n",
    "        \n",
    "        # Append the DataFrame to the list for the respective year\n",
    "        df_y[year].append(df)\n",
    "\n",
    "    print('concatenating')\n",
    "    start_time = time.time()\n",
    "    for i, (year, df_list) in enumerate(df_y.items()):\n",
    "        print(year, remaining_time(start_time, len(df_y), i))\n",
    "        df_y[year] = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    return df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_by_year dict\n",
    "def group_by_year(dataframes):\n",
    "    demo_cvm = [df for df in dataframes if len(df) > 0 and ('con' in df['FILE_NAME'][0] or 'ind' in df['FILE_NAME'][0])]\n",
    "    links = [df for df in dataframes if len(df) > 0 and ('con' not in df['FILE_NAME'][0] and 'ind' not in df['FILE_NAME'][0])]\n",
    "\n",
    "    demo_cvm = yearly(demo_cvm)\n",
    "    links = yearly(links)\n",
    "\n",
    "    # print('clean up dataframes')\n",
    "    demo_cvm = clean_dataframes(demo_cvm)\n",
    "    links = clean_dataframes(links)\n",
    "\n",
    "    # Rename column for consistency\n",
    "    for year in links.keys():\n",
    "        links[year].rename(columns={'VERSAO': 'VERSAO_LINK'}, inplace=True)\n",
    "    \n",
    "    return demo_cvm, links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função auxiliar para download pdf\n",
    "def download_pdf(df, url):\n",
    "    # Base directory to save PDFs\n",
    "    output_dir = 'assets/pdf'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    total_size = 0  # Initialize cumulative total size\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        response = requests.get(url.format(ID_DOC=row['ID_DOC']))\n",
    "        if response.status_code == 200:\n",
    "            with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
    "                file_size = len(response.content)/(1024 ** 2)\n",
    "                total_size += file_size\n",
    "                for file_info in zip_ref.infolist():\n",
    "                    if file_info.filename.lower().endswith('.pdf'):\n",
    "                        pdf_content = zip_ref.read(file_info.filename)\n",
    "                        filename = f\"{clean_text(row['DENOM_CIA'])} {row['DT_REFER']} VERSAO_{row['VERSAO']} {row['ID_DOC']}.pdf\"\n",
    "                        filepath = os.path.join(output_dir, filename)\n",
    "                        with open(filepath, 'wb') as pdf_file:\n",
    "                            pdf_file.write(pdf_content)\n",
    "                        print(f'{i+1}/{len(df)}, {filename}, {file_size:.3f} Mb, {total_size:.3f} Mb total')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of companies by AGRUPAMENTO ['ind', 'con']\n",
    "def get_companies_by_str_port(df):\n",
    "    # str_port = structured report = relatório estruturado\n",
    "    \n",
    "    # Create a pivot table to count the occurrences of 'ind' and 'con' for each 'DENOM_CIA' and 'DT_REFER'\n",
    "    pivot_table = df.pivot_table(index=['DENOM_CIA', 'DT_REFER'], columns='AGRUPAMENTO', aggfunc='size', fill_value=0)\n",
    "\n",
    "    # Apply a conditional mapping to convert counts to 1 if count > 0, and 0 otherwise\n",
    "    pivot_table = pivot_table.applymap(lambda x: True if x > 0 else False)\n",
    "    pivot_table = pivot_table[['ind'] + [col for col in pivot_table.columns if col != 'ind' and col != 'con'] + ['con']]\n",
    "\n",
    "    # Get the unique combinations of rows as tuples\n",
    "    combinations = set(map(tuple, pivot_table.to_numpy()))\n",
    "\n",
    "    # Create a dictionary to store the combinations of 'con' and 'ind' as keys and corresponding 'DENOM_CIA' as values\n",
    "    companies_by_str_port  = {}\n",
    "\n",
    "    # Iterate through the unique combinations and find matching 'DENOM_CIA'\n",
    "    for combination in combinations:\n",
    "        relest_individual = combination[0]\n",
    "        relest_consolidado = combination[1]\n",
    "        cias = pivot_table[(pivot_table['ind'] == relest_individual) & (pivot_table['con'] == relest_consolidado)].index.get_level_values('DENOM_CIA').unique()\n",
    "        key = ('ind', 'con')\n",
    "        if relest_consolidado and not relest_individual:\n",
    "            key = 'con'\n",
    "        if not relest_consolidado and relest_individual:\n",
    "            key = 'ind'\n",
    "\n",
    "        companies_by_str_port [key] = cias\n",
    "\n",
    "    return companies_by_str_port"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test API and macrodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wbdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wbdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(wbdata.get_data('NY.GDP.MKTP.CD'))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = 'https://www.tesourodireto.com.br/titulos/precos-e-taxas.htm'\n",
    "response = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Buscando os títulos e rendimentos\n",
    "titulos = soup.find_all('td', class_='listing__col--title')\n",
    "rendimentos = soup.find_all('td', class_='listing__col--rate')\n",
    "\n",
    "for titulo, rendimento in zip(titulos, rendimentos):\n",
    "    print(titulo.text.strip(), \"-\", rendimento.text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the CSV file\n",
    "url = 'https://www.tesourotransparente.gov.br/ckan/dataset/df56aa42-484a-4a59-8184-7676580c81e3/resource/796d2059-14e9-44e3-80c9-2d9e30b405c1/download/PrecoTaxaTesouroDireto.csv'\n",
    "\n",
    "# Read the CSV file with the specified separator and decimal point\n",
    "td_hist = pd.read_csv(url, sep=';', decimal=',')\n",
    "td_hist.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_groups = td_hist.groupby(['Tipo Titulo', 'Data Vencimento'])\n",
    "for group, df in td_groups:\n",
    "    print(group)\n",
    "    # df[['Data Base', 'Taxa Compra Manha']].set_index('Data Base').plot()\n",
    "len(td_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distritos = pd.read_json('https://servicodados.ibge.gov.br/api/v1/localidades/distritos')\n",
    "pesquisas = pd.read_json('http://servicodados.ibge.gov.br/api/v2/metadados/pesquisas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distritos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_pre x df: remover older, update  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_quarters = ['3', '4']\n",
    "all_quarters = ['6', '7']\n",
    "start_time = time.time()\n",
    "app_folder = 'datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(dict_of_df):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses DataFrames in a dictionary.\n",
    "\n",
    "    This function takes a dictionary of DataFrames and performs various cleaning operations on each DataFrame.\n",
    "    It removes extra rows, cleans up text columns, converts specified columns to datetime format,\n",
    "    and applies a text cleaning function to specific columns.\n",
    "\n",
    "    Args:\n",
    "        dict_of_df (dict): A dictionary where keys are years and values are DataFrames.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with cleaned and preprocessed DataFrames.\n",
    "    \"\"\"\n",
    "    # Change data types for columns\n",
    "    category_columns = ['FILENAME', 'DEMONSTRATIVO', 'BALANCE_SHEET', 'ANO', 'AGRUPAMENTO', 'CNPJ_CIA', 'VERSAO', 'DENOM_CIA', 'CD_CVM', 'GRUPO_DFP', 'MOEDA', 'ESCALA_MOEDA', 'CD_CONTA', 'DS_CONTA','ST_CONTA_FIXA', 'COLUNA_DF', ]\n",
    "    datetime_columns = ['DT_REFER', 'DT_FIM_EXERC', 'DT_INI_EXERC', ]\n",
    "    numeric_columns = ['VL_CONTA', ]\n",
    "\n",
    "    print('... cleaning database')\n",
    "    start_time = time.time()\n",
    "    for i, (year, df) in enumerate(dict_of_df.items()):\n",
    "        print(year, remaining_time(start_time, len(dict_of_df), i))\n",
    "        \n",
    "        # Remove extra rows based on specific conditions\n",
    "        try:\n",
    "            df = df[df['ORDEM_EXERC'] == 'ÚLTIMO']\n",
    "            df = df.drop(columns=['ORDEM_EXERC'])\n",
    "        except Exception as e:\n",
    "            # print(e)\n",
    "            pass\n",
    "        \n",
    "        # Apply the condition to filter the DataFrame\n",
    "        try:\n",
    "            df['DT_INI_EXERC'] = pd.to_datetime(df['DT_INI_EXERC'], errors='coerce')\n",
    "            mask = (df['DT_INI_EXERC'].dt.month == 1) | (df['DT_INI_EXERC'].isna())\n",
    "            df = df[mask].copy()  # Make a copy to avoid modifying the original DataFrame\n",
    "            df = df.drop(columns=['DT_INI_EXERC'])\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "        # Clean up text in 'DENOM_CIA' column\n",
    "        try:\n",
    "            df['DENOM_CIA'] = df['DENOM_CIA'].apply(clean_text)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "        # Convert specified columns to specified formats\n",
    "        for column in df.columns:\n",
    "            if column in category_columns:\n",
    "                try:\n",
    "                    df[column] = df[column].astype('category')\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "            elif column in datetime_columns:\n",
    "                try:\n",
    "                    df[column] = pd.to_datetime(df[column])\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "            elif column in numeric_columns:\n",
    "                try:\n",
    "                    df[column] = pd.to_numeric(df[column], errors='ignore')\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "\n",
    "        dict_of_df[year] = df\n",
    "\n",
    "    return dict_of_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_math_magic(demo_cvm, max_iterations=20000000):\n",
    "    \"\"\"\n",
    "    Perform 'magic' calculations on the DataFrame demo_cvm based on specified quarters.\n",
    "\n",
    "    Args:\n",
    "        demo_cvm (dict): Dictionary of DataFrames containing financial data.\n",
    "        last_quarters (list): List of quarters considered as last quarters.\n",
    "        all_quarters (list): List of quarters considered as all quarters.\n",
    "        max_iterations (int): Maximum number of iterations to perform.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated demo_cvm with 'magic' calculations.\n",
    "\n",
    "    This function iterates through the provided demo_cvm DataFrames, performs calculations based on specified quarters,\n",
    "    and updates the 'VL_CONTA' values where necessary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print('entering the smart mathmagic world... It takes long time, came back tomorrow... ')\n",
    "        start_time = time.time()\n",
    "        # Iterate through each year's DataFrame\n",
    "        for n1, (year, demonstrativo_cvm) in enumerate(demo_cvm.items()):\n",
    "            companies_by_str_port = get_companies_by_str_port(demonstrativo_cvm)\n",
    "            print(f\"{year} {len(demonstrativo_cvm):,.0f} lines, {len(demonstrativo_cvm['DENOM_CIA'].unique())} companies, {'/'.join([f'{len(companies)} {key}' for key, companies in companies_by_str_port.items()])}\")\n",
    "            print(year, remaining_time(start_time, len(demo_cvm), n1))\n",
    "            # Convert DT_REFER to datetime\n",
    "            demonstrativo_cvm['DT_REFER'] = pd.to_datetime(demonstrativo_cvm['DT_REFER'])\n",
    "            groups = demonstrativo_cvm.groupby(['DENOM_CIA', 'AGRUPAMENTO'], group_keys=False)\n",
    "            start_time_2 = time.time()\n",
    "            for n2, (key, group) in enumerate(groups):\n",
    "            #   if key[0] == 'ALPARGATAS SA':\n",
    "                print('  ', remaining_time(start_time_2, len(groups), n2))\n",
    "                company = key[0]\n",
    "                agg = key[1]\n",
    "                subgroups = group.groupby(['CD_CONTA', 'DS_CONTA'], group_keys=False)\n",
    "                \n",
    "                start_time_3 = time.time()\n",
    "                for n3, (index, df) in enumerate(subgroups):\n",
    "                    # print('  ', '  ', remaining_time(start_time_3, len(subgroups), n3))\n",
    "                    conta_first = index[0][0]\n",
    "                   \n",
    "                    try:\n",
    "                        i1 = df[df['DT_REFER'].dt.quarter == 1].index[0]\n",
    "                        q1 = df[df['DT_REFER'].dt.quarter == 1]['VL_CONTA'].iloc[0]\n",
    "                    except Exception:\n",
    "                        q1 = 0\n",
    "                    try:\n",
    "                        i2 = df[df['DT_REFER'].dt.quarter == 2].index[0]\n",
    "                        q2 = df[df['DT_REFER'].dt.quarter == 2]['VL_CONTA'].iloc[0]\n",
    "                    except Exception:\n",
    "                        q2 = 0\n",
    "                    try:\n",
    "                        i3 = df[df['DT_REFER'].dt.quarter == 3].index[0]\n",
    "                        q3 = df[df['DT_REFER'].dt.quarter == 3]['VL_CONTA'].iloc[0]\n",
    "                    except Exception:\n",
    "                        q3 = 0\n",
    "                    try:\n",
    "                        i4 = df[df['DT_REFER'].dt.quarter == 4].index[0]\n",
    "                        q4 = df[df['DT_REFER'].dt.quarter == 4]['VL_CONTA'].iloc[0]\n",
    "                    except Exception:\n",
    "                        q4 = 0\n",
    "\n",
    "                    update = False\n",
    "                    try:\n",
    "                        # Perform calculations based on specified quarters and update flag\n",
    "                        if conta_first in last_quarters and i4:\n",
    "                            q4 = q4 - (q3)\n",
    "                            update = True\n",
    "                        elif conta_first in all_quarters and i2 and i3 and i4:\n",
    "                            q4 = q4 - (q3)\n",
    "                            q3 = q3 - (q2)\n",
    "                            q2 = q2 - (q1)\n",
    "                            update = True\n",
    "                    except Exception as e:\n",
    "                        update = False\n",
    "\n",
    "                    if update:\n",
    "                        # Update 'VL_CONTA' values\n",
    "                        try:\n",
    "                           demonstrativo_cvm.loc[i1, 'VL_CONTA'] = q1\n",
    "                        except Exception as e:\n",
    "                            pass\n",
    "                        try:\n",
    "                           demonstrativo_cvm.loc[i2, 'VL_CONTA'] = q2\n",
    "                        except Exception as e:\n",
    "                            pass\n",
    "                        try:\n",
    "                           demonstrativo_cvm.loc[i3, 'VL_CONTA'] = q3\n",
    "                        except Exception as e:\n",
    "                            pass\n",
    "                        try:\n",
    "                           demonstrativo_cvm.loc[i4, 'VL_CONTA'] = q4\n",
    "                        except Exception as e:\n",
    "                            pass\n",
    "\n",
    "                    if n3 > max_iterations:\n",
    "                        break\n",
    "                if n2 > max_iterations:\n",
    "                    break\n",
    "            demo_cvm[year] = demonstrativo_cvm\n",
    "            if n1 > max_iterations:\n",
    "                break\n",
    "    except Exception as e:\n",
    "       pass\n",
    "    return demo_cvm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvm_now = load_pkl(f'{app_folder}cvm_now')\n",
    "cvm_new = load_pkl(f'{app_folder}cvm_new')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all unique years from both dictionaries\n",
    "all_keys = set(cvm_now.keys()).union(cvm_new.keys())\n",
    "cols =        ['DEMONSTRATIVO', 'BALANCE_SHEET', 'ANO', 'AGRUPAMENTO', 'CNPJ_CIA', 'DT_REFER', 'DENOM_CIA', 'DT_INI_EXERC', 'DT_FIM_EXERC', 'CD_CONTA', 'DS_CONTA', 'VL_CONTA']\n",
    "\n",
    "key_columns = ['FILENAME', 'DEMONSTRATIVO', 'BALANCE_SHEET', 'ANO', 'AGRUPAMENTO', 'CNPJ_CIA', 'DT_REFER', 'VERSAO', 'DENOM_CIA', 'CD_CVM', 'GRUPO_DFP', 'MOEDA', 'ESCALA_MOEDA', 'DT_INI_EXERC', 'DT_FIM_EXERC', 'CD_CONTA', 'DS_CONTA', 'ST_CONTA_FIXA', 'COLUNA_DF']\n",
    "\n",
    "cvm_merged = {}\n",
    "cvm_all_year_rows = {}\n",
    "\n",
    "for year in all_keys:\n",
    "    try:\n",
    "        df_columns = cvm_now[year].columns\n",
    "        half = int(len(cvm_now[year])/2)\n",
    "    except Exception as e:\n",
    "        df_columns = cvm_new[year].columns\n",
    "        half = int(len(cvm_new[year])/2)\n",
    "\n",
    "    df1 = cvm_now.get(year, pd.DataFrame(columns=df_columns))\n",
    "    df2 = cvm_new.get(year, pd.DataFrame(columns=df_columns))\n",
    "    \n",
    "    # for debug purposes\n",
    "    df1 = df2[:-100].copy()\n",
    "    df1.loc[:,'VL_CONTA'] = df1.loc[:,'VL_CONTA']/2\n",
    "    mask = (df2['DENOM_CIA'] == 'ALPARGATAS SA') & \\\n",
    "        (df2['AGRUPAMENTO'] == 'con') & \\\n",
    "        (df2['CD_CONTA'].isin(['1.01', '3.01', '6.01']))\n",
    "    df2 = df2[mask]\n",
    "    \n",
    "    # Check if df1 doesn't exist and df2 exists\n",
    "    if df1.shape[0] == 0 and df2.shape[0] != 0:\n",
    "        cvm_merged[year] = df2\n",
    "\n",
    "    # Check if df2 doesn't exist and df1 exists\n",
    "    elif df2.shape[0] == 0 and df1.shape[0] != 0:\n",
    "        cvm_merged[year] = df1\n",
    "\n",
    "    # Both df1 and df2 exist (because at least one key exists)\n",
    "    else:\n",
    "        df_merged = pd.merge(df1, df2, on=key_columns, how='outer', indicator=True, suffixes=('_now', '_new'))\n",
    "        \n",
    "        # Create df_diff to only contain rows from df2\n",
    "        diff_rows = df_merged[df_merged['_merge'].isin(['right_only', 'both'])].copy()\n",
    "\n",
    "        # Conditional update of the 'VL_CONTA_new' column based on the '_merge' column value\n",
    "        df_merged['VL_CONTA_new'] = np.where(df_merged['_merge'] == 'left_only', df_merged['VL_CONTA_now'], df_merged['VL_CONTA_new'])\n",
    "\n",
    "        # Drop the 'VL_CONTA_now' and '_merge' columns\n",
    "        df_merged.drop(columns=['VL_CONTA_now', '_merge'], inplace=True)\n",
    "        \n",
    "        # Rename the 'VL_CONTA_new' column to 'VL_CONTA'\n",
    "        df_merged.rename(columns={'VL_CONTA_new': 'VL_CONTA'}, inplace=True)\n",
    "        df_merged = df_merged[df_columns]\n",
    "\n",
    "        # for index, row in diff_rows.iterrows():\n",
    "        #     refer_year = pd.to_datetime(row['DT_REFER']).year  # Extract the year from DT_REFER\n",
    "        #     cvm_all_year_rows[refer_year] = df_merged[df_merged['DT_REFER'].dt.year == refer_year]\n",
    "\n",
    "\n",
    "        cvm_merged[year] = df_merged\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_quarters(group_df):\n",
    "    # Filtering to get VL_CONTA values for each quarter\n",
    "    q1_value = group_df.loc[group_df['DT_REFER'].dt.month == 3, 'VL_CONTA'].values[0] if not group_df.loc[group_df['DT_REFER'].dt.month == 3, 'VL_CONTA'].empty else 0\n",
    "    q2_value = group_df.loc[group_df['DT_REFER'].dt.month == 6, 'VL_CONTA'].values[0] if not group_df.loc[group_df['DT_REFER'].dt.month == 6, 'VL_CONTA'].empty else 0\n",
    "    q3_value = group_df.loc[group_df['DT_REFER'].dt.month == 9, 'VL_CONTA'].values[0] if not group_df.loc[group_df['DT_REFER'].dt.month == 9, 'VL_CONTA'].empty else 0\n",
    "    q4_value = group_df.loc[group_df['DT_REFER'].dt.month == 12, 'VL_CONTA'].values[0] if not group_df.loc[group_df['DT_REFER'].dt.month == 12, 'VL_CONTA'].empty else 0\n",
    "\n",
    "    # Adjusting VL_CONTA values as per the formulas\n",
    "    q4_adj = q4_value - (q3_value + q2_value + q1_value)\n",
    "    q3_adj = q3_value - (q2_value + q1_value)\n",
    "    q2_adj = q2_value - q1_value\n",
    "\n",
    "    # Assigning the adjusted values back to the DataFrame\n",
    "    group_df.loc[group_df['DT_REFER'].dt.month == 12, 'VL_CONTA'] = q4_adj\n",
    "    group_df.loc[group_df['DT_REFER'].dt.month == 9, 'VL_CONTA'] = q3_adj\n",
    "    group_df.loc[group_df['DT_REFER'].dt.month == 6, 'VL_CONTA'] = q2_adj\n",
    "\n",
    "    return group_df\n",
    "\n",
    "def filter_last_quarter(group_df):\n",
    "    # For Q2 and Q3, only keep rows where DT_REFER and DT_INI_EXERC quarters match\n",
    "    mask = ~group_df['DT_REFER'].dt.quarter.isin([2, 3]) | (group_df['DT_REFER'].dt.quarter == group_df['DT_INI_EXERC'].dt.quarter)\n",
    "    group_df = group_df[mask]\n",
    "    return group_df\n",
    "\n",
    "def adjust_last_quarter(group_df):\n",
    "    # Adjust Q4 values\n",
    "    q1_value = group_df[group_df['DT_REFER'].dt.quarter == 1]['VL_CONTA'].sum()\n",
    "    q2_value = group_df[group_df['DT_REFER'].dt.quarter == 2]['VL_CONTA'].sum()\n",
    "    q3_value = group_df[group_df['DT_REFER'].dt.quarter == 3]['VL_CONTA'].sum()\n",
    "    \n",
    "    mask = group_df['DT_REFER'].dt.quarter == 4\n",
    "    group_df.loc[mask, 'VL_CONTA'] -= (q1_value + q2_value + q3_value)\n",
    "    return group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store all the filtered rows\n",
    "all_filtered_rows = []\n",
    "\n",
    "for index, row in diff_rows.iterrows():\n",
    "    # Initialize a filter mask\n",
    "    mask = (df_merged['DENOM_CIA'] == row['DENOM_CIA']) & \\\n",
    "           (df_merged['AGRUPAMENTO'] == row['AGRUPAMENTO']) & \\\n",
    "           (df_merged['CD_CONTA'] == row['CD_CONTA']) & \\\n",
    "           (df_merged['DS_CONTA'] == row['DS_CONTA'])\n",
    "\n",
    "    # Get the filtered rows using the mask\n",
    "    filtered_rows = df_merged[mask].copy()\n",
    "\n",
    "    # Check if the BALANCE_SHEET value is one of the 'all_quarters'\n",
    "    if row['BALANCE_SHEET'] in all_quarters:\n",
    "        adjusted_rows = adjust_quarters(filtered_rows)\n",
    "        all_filtered_rows.append(adjusted_rows)\n",
    "    elif row['BALANCE_SHEET'] in last_quarter:\n",
    "        # Filter and adjust for last quarter\n",
    "        filtered_rows = filter_last_quarter(filtered_rows)\n",
    "        adjusted_rows = adjust_last_quarter(filtered_rows)\n",
    "        all_filtered_rows.append(adjusted_rows)\n",
    "    else:\n",
    "        all_filtered_rows.append(filtered_rows)\n",
    "\n",
    "# Concatenate all the rows (adjusted and non-adjusted) into a single DataFrame\n",
    "math = pd.concat(all_filtered_rows, axis=0).drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILENAME</th>\n",
       "      <th>DEMONSTRATIVO</th>\n",
       "      <th>BALANCE_SHEET</th>\n",
       "      <th>ANO</th>\n",
       "      <th>AGRUPAMENTO</th>\n",
       "      <th>CNPJ_CIA</th>\n",
       "      <th>DT_REFER</th>\n",
       "      <th>VERSAO</th>\n",
       "      <th>DENOM_CIA</th>\n",
       "      <th>CD_CVM</th>\n",
       "      <th>GRUPO_DFP</th>\n",
       "      <th>MOEDA</th>\n",
       "      <th>ESCALA_MOEDA</th>\n",
       "      <th>DT_FIM_EXERC</th>\n",
       "      <th>CD_CONTA</th>\n",
       "      <th>DS_CONTA</th>\n",
       "      <th>VL_CONTA</th>\n",
       "      <th>ST_CONTA_FIXA</th>\n",
       "      <th>DT_INI_EXERC</th>\n",
       "      <th>COLUNA_DF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49328</th>\n",
       "      <td>itr_cia_aberta_BPA_con_2016.csv</td>\n",
       "      <td>itr</td>\n",
       "      <td>BPA</td>\n",
       "      <td>2016</td>\n",
       "      <td>con</td>\n",
       "      <td>61.079.117/0001-05</td>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>1</td>\n",
       "      <td>ALPARGATAS SA</td>\n",
       "      <td>10456</td>\n",
       "      <td>DF Consolidado - Balanço Patrimonial Ativo</td>\n",
       "      <td>REAL</td>\n",
       "      <td>MIL</td>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>1.01</td>\n",
       "      <td>Ativo Circulante</td>\n",
       "      <td>2150935.0</td>\n",
       "      <td>S</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49395</th>\n",
       "      <td>itr_cia_aberta_BPA_con_2016.csv</td>\n",
       "      <td>itr</td>\n",
       "      <td>BPA</td>\n",
       "      <td>2016</td>\n",
       "      <td>con</td>\n",
       "      <td>61.079.117/0001-05</td>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>2</td>\n",
       "      <td>ALPARGATAS SA</td>\n",
       "      <td>10456</td>\n",
       "      <td>DF Consolidado - Balanço Patrimonial Ativo</td>\n",
       "      <td>REAL</td>\n",
       "      <td>MIL</td>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>1.01</td>\n",
       "      <td>Ativo Circulante</td>\n",
       "      <td>2260506.0</td>\n",
       "      <td>S</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49462</th>\n",
       "      <td>itr_cia_aberta_BPA_con_2016.csv</td>\n",
       "      <td>itr</td>\n",
       "      <td>BPA</td>\n",
       "      <td>2016</td>\n",
       "      <td>con</td>\n",
       "      <td>61.079.117/0001-05</td>\n",
       "      <td>2016-09-30</td>\n",
       "      <td>1</td>\n",
       "      <td>ALPARGATAS SA</td>\n",
       "      <td>10456</td>\n",
       "      <td>DF Consolidado - Balanço Patrimonial Ativo</td>\n",
       "      <td>REAL</td>\n",
       "      <td>MIL</td>\n",
       "      <td>2016-09-30</td>\n",
       "      <td>1.01</td>\n",
       "      <td>Ativo Circulante</td>\n",
       "      <td>2179017.0</td>\n",
       "      <td>S</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431027</th>\n",
       "      <td>dfp_cia_aberta_BPA_con_2016.csv</td>\n",
       "      <td>dfp</td>\n",
       "      <td>BPA</td>\n",
       "      <td>2016</td>\n",
       "      <td>con</td>\n",
       "      <td>61.079.117/0001-05</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>2</td>\n",
       "      <td>ALPARGATAS SA</td>\n",
       "      <td>10456</td>\n",
       "      <td>DF Consolidado - Balanço Patrimonial Ativo</td>\n",
       "      <td>REAL</td>\n",
       "      <td>MIL</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>1.01</td>\n",
       "      <td>Ativo Circulante</td>\n",
       "      <td>2262005.0</td>\n",
       "      <td>S</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521593</th>\n",
       "      <td>itr_cia_aberta_DFC_MI_con_2016.csv</td>\n",
       "      <td>itr</td>\n",
       "      <td>DFC_MI</td>\n",
       "      <td>2016</td>\n",
       "      <td>con</td>\n",
       "      <td>61.079.117/0001-05</td>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>1</td>\n",
       "      <td>ALPARGATAS SA</td>\n",
       "      <td>10456</td>\n",
       "      <td>DF Consolidado - Demonstração do Fluxo de Caixa (Método Indireto)</td>\n",
       "      <td>REAL</td>\n",
       "      <td>MIL</td>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>6.01</td>\n",
       "      <td>Caixa Líquido Atividades Operacionais</td>\n",
       "      <td>-8520.0</td>\n",
       "      <td>S</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521634</th>\n",
       "      <td>itr_cia_aberta_DFC_MI_con_2016.csv</td>\n",
       "      <td>itr</td>\n",
       "      <td>DFC_MI</td>\n",
       "      <td>2016</td>\n",
       "      <td>con</td>\n",
       "      <td>61.079.117/0001-05</td>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>2</td>\n",
       "      <td>ALPARGATAS SA</td>\n",
       "      <td>10456</td>\n",
       "      <td>DF Consolidado - Demonstração do Fluxo de Caixa (Método Indireto)</td>\n",
       "      <td>REAL</td>\n",
       "      <td>MIL</td>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>6.01</td>\n",
       "      <td>Caixa Líquido Atividades Operacionais</td>\n",
       "      <td>98487.0</td>\n",
       "      <td>S</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521675</th>\n",
       "      <td>itr_cia_aberta_DFC_MI_con_2016.csv</td>\n",
       "      <td>itr</td>\n",
       "      <td>DFC_MI</td>\n",
       "      <td>2016</td>\n",
       "      <td>con</td>\n",
       "      <td>61.079.117/0001-05</td>\n",
       "      <td>2016-09-30</td>\n",
       "      <td>1</td>\n",
       "      <td>ALPARGATAS SA</td>\n",
       "      <td>10456</td>\n",
       "      <td>DF Consolidado - Demonstração do Fluxo de Caixa (Método Indireto)</td>\n",
       "      <td>REAL</td>\n",
       "      <td>MIL</td>\n",
       "      <td>2016-09-30</td>\n",
       "      <td>6.01</td>\n",
       "      <td>Caixa Líquido Atividades Operacionais</td>\n",
       "      <td>95266.0</td>\n",
       "      <td>S</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594714</th>\n",
       "      <td>dfp_cia_aberta_DFC_MI_con_2016.csv</td>\n",
       "      <td>dfp</td>\n",
       "      <td>DFC_MI</td>\n",
       "      <td>2016</td>\n",
       "      <td>con</td>\n",
       "      <td>61.079.117/0001-05</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>2</td>\n",
       "      <td>ALPARGATAS SA</td>\n",
       "      <td>10456</td>\n",
       "      <td>DF Consolidado - Demonstração do Fluxo de Caixa (Método Indireto)</td>\n",
       "      <td>REAL</td>\n",
       "      <td>MIL</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>6.01</td>\n",
       "      <td>Caixa Líquido Atividades Operacionais</td>\n",
       "      <td>82811.0</td>\n",
       "      <td>S</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186273</th>\n",
       "      <td>itr_cia_aberta_DRE_con_2016.csv</td>\n",
       "      <td>itr</td>\n",
       "      <td>DRE</td>\n",
       "      <td>2016</td>\n",
       "      <td>con</td>\n",
       "      <td>61.079.117/0001-05</td>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>1</td>\n",
       "      <td>ALPARGATAS SA</td>\n",
       "      <td>10456</td>\n",
       "      <td>DF Consolidado - Demonstração do Resultado</td>\n",
       "      <td>REAL</td>\n",
       "      <td>MIL</td>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>3.01</td>\n",
       "      <td>Receita de Venda de Bens e/ou Serviços</td>\n",
       "      <td>1011568.0</td>\n",
       "      <td>S</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186310</th>\n",
       "      <td>itr_cia_aberta_DRE_con_2016.csv</td>\n",
       "      <td>itr</td>\n",
       "      <td>DRE</td>\n",
       "      <td>2016</td>\n",
       "      <td>con</td>\n",
       "      <td>61.079.117/0001-05</td>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>2</td>\n",
       "      <td>ALPARGATAS SA</td>\n",
       "      <td>10456</td>\n",
       "      <td>DF Consolidado - Demonstração do Resultado</td>\n",
       "      <td>REAL</td>\n",
       "      <td>MIL</td>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>3.01</td>\n",
       "      <td>Receita de Venda de Bens e/ou Serviços</td>\n",
       "      <td>1011899.0</td>\n",
       "      <td>S</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186382</th>\n",
       "      <td>itr_cia_aberta_DRE_con_2016.csv</td>\n",
       "      <td>itr</td>\n",
       "      <td>DRE</td>\n",
       "      <td>2016</td>\n",
       "      <td>con</td>\n",
       "      <td>61.079.117/0001-05</td>\n",
       "      <td>2016-09-30</td>\n",
       "      <td>1</td>\n",
       "      <td>ALPARGATAS SA</td>\n",
       "      <td>10456</td>\n",
       "      <td>DF Consolidado - Demonstração do Resultado</td>\n",
       "      <td>REAL</td>\n",
       "      <td>MIL</td>\n",
       "      <td>2016-09-30</td>\n",
       "      <td>3.01</td>\n",
       "      <td>Receita de Venda de Bens e/ou Serviços</td>\n",
       "      <td>982831.0</td>\n",
       "      <td>S</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822975</th>\n",
       "      <td>dfp_cia_aberta_DRE_con_2016.csv</td>\n",
       "      <td>dfp</td>\n",
       "      <td>DRE</td>\n",
       "      <td>2016</td>\n",
       "      <td>con</td>\n",
       "      <td>61.079.117/0001-05</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>2</td>\n",
       "      <td>ALPARGATAS SA</td>\n",
       "      <td>10456</td>\n",
       "      <td>DF Consolidado - Demonstração do Resultado</td>\n",
       "      <td>REAL</td>\n",
       "      <td>MIL</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>3.01</td>\n",
       "      <td>Receita de Venda de Bens e/ou Serviços</td>\n",
       "      <td>1048106.0</td>\n",
       "      <td>S</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   FILENAME DEMONSTRATIVO BALANCE_SHEET   ANO  \\\n",
       "49328       itr_cia_aberta_BPA_con_2016.csv           itr           BPA  2016   \n",
       "49395       itr_cia_aberta_BPA_con_2016.csv           itr           BPA  2016   \n",
       "49462       itr_cia_aberta_BPA_con_2016.csv           itr           BPA  2016   \n",
       "1431027     dfp_cia_aberta_BPA_con_2016.csv           dfp           BPA  2016   \n",
       "521593   itr_cia_aberta_DFC_MI_con_2016.csv           itr        DFC_MI  2016   \n",
       "521634   itr_cia_aberta_DFC_MI_con_2016.csv           itr        DFC_MI  2016   \n",
       "521675   itr_cia_aberta_DFC_MI_con_2016.csv           itr        DFC_MI  2016   \n",
       "1594714  dfp_cia_aberta_DFC_MI_con_2016.csv           dfp        DFC_MI  2016   \n",
       "1186273     itr_cia_aberta_DRE_con_2016.csv           itr           DRE  2016   \n",
       "1186310     itr_cia_aberta_DRE_con_2016.csv           itr           DRE  2016   \n",
       "1186382     itr_cia_aberta_DRE_con_2016.csv           itr           DRE  2016   \n",
       "1822975     dfp_cia_aberta_DRE_con_2016.csv           dfp           DRE  2016   \n",
       "\n",
       "        AGRUPAMENTO            CNPJ_CIA   DT_REFER VERSAO      DENOM_CIA  \\\n",
       "49328           con  61.079.117/0001-05 2016-03-31      1  ALPARGATAS SA   \n",
       "49395           con  61.079.117/0001-05 2016-06-30      2  ALPARGATAS SA   \n",
       "49462           con  61.079.117/0001-05 2016-09-30      1  ALPARGATAS SA   \n",
       "1431027         con  61.079.117/0001-05 2016-12-31      2  ALPARGATAS SA   \n",
       "521593          con  61.079.117/0001-05 2016-03-31      1  ALPARGATAS SA   \n",
       "521634          con  61.079.117/0001-05 2016-06-30      2  ALPARGATAS SA   \n",
       "521675          con  61.079.117/0001-05 2016-09-30      1  ALPARGATAS SA   \n",
       "1594714         con  61.079.117/0001-05 2016-12-31      2  ALPARGATAS SA   \n",
       "1186273         con  61.079.117/0001-05 2016-03-31      1  ALPARGATAS SA   \n",
       "1186310         con  61.079.117/0001-05 2016-06-30      2  ALPARGATAS SA   \n",
       "1186382         con  61.079.117/0001-05 2016-09-30      1  ALPARGATAS SA   \n",
       "1822975         con  61.079.117/0001-05 2016-12-31      2  ALPARGATAS SA   \n",
       "\n",
       "        CD_CVM  \\\n",
       "49328    10456   \n",
       "49395    10456   \n",
       "49462    10456   \n",
       "1431027  10456   \n",
       "521593   10456   \n",
       "521634   10456   \n",
       "521675   10456   \n",
       "1594714  10456   \n",
       "1186273  10456   \n",
       "1186310  10456   \n",
       "1186382  10456   \n",
       "1822975  10456   \n",
       "\n",
       "                                                                 GRUPO_DFP  \\\n",
       "49328                           DF Consolidado - Balanço Patrimonial Ativo   \n",
       "49395                           DF Consolidado - Balanço Patrimonial Ativo   \n",
       "49462                           DF Consolidado - Balanço Patrimonial Ativo   \n",
       "1431027                         DF Consolidado - Balanço Patrimonial Ativo   \n",
       "521593   DF Consolidado - Demonstração do Fluxo de Caixa (Método Indireto)   \n",
       "521634   DF Consolidado - Demonstração do Fluxo de Caixa (Método Indireto)   \n",
       "521675   DF Consolidado - Demonstração do Fluxo de Caixa (Método Indireto)   \n",
       "1594714  DF Consolidado - Demonstração do Fluxo de Caixa (Método Indireto)   \n",
       "1186273                         DF Consolidado - Demonstração do Resultado   \n",
       "1186310                         DF Consolidado - Demonstração do Resultado   \n",
       "1186382                         DF Consolidado - Demonstração do Resultado   \n",
       "1822975                         DF Consolidado - Demonstração do Resultado   \n",
       "\n",
       "        MOEDA ESCALA_MOEDA DT_FIM_EXERC CD_CONTA  \\\n",
       "49328    REAL          MIL   2016-03-31     1.01   \n",
       "49395    REAL          MIL   2016-06-30     1.01   \n",
       "49462    REAL          MIL   2016-09-30     1.01   \n",
       "1431027  REAL          MIL   2016-12-31     1.01   \n",
       "521593   REAL          MIL   2016-03-31     6.01   \n",
       "521634   REAL          MIL   2016-06-30     6.01   \n",
       "521675   REAL          MIL   2016-09-30     6.01   \n",
       "1594714  REAL          MIL   2016-12-31     6.01   \n",
       "1186273  REAL          MIL   2016-03-31     3.01   \n",
       "1186310  REAL          MIL   2016-06-30     3.01   \n",
       "1186382  REAL          MIL   2016-09-30     3.01   \n",
       "1822975  REAL          MIL   2016-12-31     3.01   \n",
       "\n",
       "                                       DS_CONTA   VL_CONTA ST_CONTA_FIXA  \\\n",
       "49328                          Ativo Circulante  2150935.0             S   \n",
       "49395                          Ativo Circulante  2260506.0             S   \n",
       "49462                          Ativo Circulante  2179017.0             S   \n",
       "1431027                        Ativo Circulante  2262005.0             S   \n",
       "521593    Caixa Líquido Atividades Operacionais    -8520.0             S   \n",
       "521634    Caixa Líquido Atividades Operacionais    98487.0             S   \n",
       "521675    Caixa Líquido Atividades Operacionais    95266.0             S   \n",
       "1594714   Caixa Líquido Atividades Operacionais    82811.0             S   \n",
       "1186273  Receita de Venda de Bens e/ou Serviços  1011568.0             S   \n",
       "1186310  Receita de Venda de Bens e/ou Serviços  1011899.0             S   \n",
       "1186382  Receita de Venda de Bens e/ou Serviços   982831.0             S   \n",
       "1822975  Receita de Venda de Bens e/ou Serviços  1048106.0             S   \n",
       "\n",
       "        DT_INI_EXERC COLUNA_DF  \n",
       "49328            NaT       NaN  \n",
       "49395            NaT       NaN  \n",
       "49462            NaT       NaN  \n",
       "1431027          NaT       NaN  \n",
       "521593    2016-01-01       NaN  \n",
       "521634    2016-01-01       NaN  \n",
       "521675    2016-01-01       NaN  \n",
       "1594714   2016-01-01       NaN  \n",
       "1186273   2016-01-01       NaN  \n",
       "1186310   2016-04-01       NaN  \n",
       "1186382   2016-07-01       NaN  \n",
       "1822975   2016-01-01       NaN  "
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cia_columns = ['DENOM_CIA']\n",
    "quarter_columns = ['DT_REFER']\n",
    "agg_columns = ['AGRUPAMENTO']\n",
    "sheet_columns = ['BALANCE_SHEET']\n",
    "content_cols = ['CD_CONTA', 'DS_CONTA', 'VL_CONTA']\n",
    "unique_columns = cia_columns + quarter_columns + agg_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "company = 'ALPARGATAS SA'\n",
    "agg = 'con'\n",
    "\n",
    "filtered_dfs = {}\n",
    "\n",
    "for year, df in df_old.items():\n",
    "    # Use boolean indexing to filter by 'DENOM_CIA' and 'AGG_COLUMN' (replace 'AGG_COLUMN' with the actual column name)\n",
    "    filtered_df = df[(df['DENOM_CIA'] == company) & (df['AGRUPAMENTO'] == agg)]\n",
    "    \n",
    "    # Store the filtered DataFrame in a new dictionary\n",
    "    filtered_dfs[year] = filtered_df\n",
    "df_old = filtered_dfs\n",
    "\n",
    "filtered_dfs = {}\n",
    "\n",
    "for year, df in df_new.items():\n",
    "    # Use boolean indexing to filter by 'DENOM_CIA' and 'AGG_COLUMN' (replace 'AGG_COLUMN' with the actual column name)\n",
    "    filtered_df = df[(df['DENOM_CIA'] == company) & (df['AGRUPAMENTO'] == agg)]\n",
    "    \n",
    "    # Store the filtered DataFrame in a new dictionary\n",
    "    filtered_dfs[year] = filtered_df\n",
    "df_new = filtered_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff = {}\n",
    "df_math = {}\n",
    "for year in df_new.keys():\n",
    "    df1 = df_old[year]\n",
    "    df2 = df_new[year]\n",
    "    if df1.shape == df2.shape:\n",
    "        mask_diff = df1['VL_CONTA'] != df2['VL_CONTA']\n",
    "        df_diff[year] = df_new[year][mask_diff]\n",
    "        for i, (idx, row) in enumerate(df_diff[year].iterrows()):\n",
    "            # Print information about the difference\n",
    "            print(\n",
    "                f\"{row['DENOM_CIA']}, {row['AGRUPAMENTO']}, {row['DT_REFER'].strftime('%Y-%m-%d')}, \"\n",
    "                f\"Old Value: {int(df1.loc[idx, 'VL_CONTA'])} -> New Value: {int(row['VL_CONTA'])}, \"\n",
    "                f\"{row['CD_CONTA']}, {row['DS_CONTA']}\")\n",
    "\n",
    "            # Create filters to identify rows in df2 that match the current difference\n",
    "            filter_mask_cia = df2['DENOM_CIA'] == row['DENOM_CIA']\n",
    "            filter_mask_agg = df2['AGRUPAMENTO'] == row['AGRUPAMENTO']\n",
    "            filter_mask_conta = df2['CD_CONTA'] == row['CD_CONTA']\n",
    "            filter_mask_year = df2['DT_REFER'].dt.year == row['DT_REFER'].year\n",
    "\n",
    "            # Combine filters to create a mask for the matching rows in df2\n",
    "            mask = filter_mask_cia & filter_mask_agg & filter_mask_conta & filter_mask_year\n",
    "\n",
    "            # Append the filtered matching rows to the list\n",
    "            df_math[year].append(df2[mask])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cia_columns = ['DENOM_CIA']\n",
    "quarter_columns = ['DT_REFER']\n",
    "agg_columns = ['AGRUPAMENTO']\n",
    "sheet_columns = ['BALANCE_SHEET']\n",
    "content_cols = ['CD_CONTA', 'DS_CONTA', 'VL_CONTA']\n",
    "unique_columns = cia_columns + quarter_columns + agg_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_cia = df_new[2014]['DENOM_CIA'] == 'ALPARGATAS SA'\n",
    "mask_agg = df_new[2014]['AGRUPAMENTO'] == 'con'\n",
    "mask_quarter = df_new[2014]['DT_REFER'] == '2014-06-30'\n",
    "mask_sheet = df_new[2014]['BALANCE_SHEET'] == 'BPA'\n",
    "mask_CD_CONTA = df_new[2014]['CD_CONTA'] == '1'\n",
    "mask = mask_cia & mask_agg & mask_quarter & mask_sheet & mask_CD_CONTA\n",
    "df_new[2014].loc[mask, 'VL_CONTA'] = 1000000.0\n",
    "\n",
    "\n",
    "mask_cia = df_new[2015]['DENOM_CIA'] == 'ALPARGATAS SA'\n",
    "mask_agg = df_new[2015]['AGRUPAMENTO'] == 'con'\n",
    "mask_quarter = df_new[2015]['DT_REFER'] == '2015-09-30'\n",
    "mask_sheet = df_new[2015]['BALANCE_SHEET'] == 'DRE'\n",
    "mask_CD_CONTA = df_new[2015]['CD_CONTA'] == '3.01'\n",
    "mask = mask_cia & mask_agg & mask_quarter & mask_sheet & mask_CD_CONTA\n",
    "df_new[2015].loc[mask, 'VL_CONTA'] = 1000000.0\n",
    "\n",
    "\n",
    "mask_cia = df_new[2016]['DENOM_CIA'] == 'ALPARGATAS SA'\n",
    "mask_agg = df_new[2016]['AGRUPAMENTO'] == 'con'\n",
    "mask_quarter = df_new[2016]['DT_REFER'] == '2016-12-31'\n",
    "mask_sheet = df_new[2016]['BALANCE_SHEET'] == 'DFC_MI'\n",
    "mask_CD_CONTA = df_new[2016]['CD_CONTA'] == '6.01'\n",
    "mask = mask_cia & mask_agg & mask_quarter & mask_sheet & mask_CD_CONTA\n",
    "df_new[2016].loc[mask, 'VL_CONTA'] = 1000000.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separa até o ano anterior ao atual e atualiza só a diferença\n",
    "# para o ano atual, faz todo o mathmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff(df1, df2):\n",
    "    # Create a mask to identify rows in df2 where 'VL_CONTA' is different from df1\n",
    "    mask_diff = df1['VL_CONTA'] != df2['VL_CONTA']\n",
    "\n",
    "    # Initialize an empty list to store DataFrames with differences\n",
    "    df_diff = []\n",
    "\n",
    "    # Iterate through rows in df2 that have differences in 'VL_CONTA'\n",
    "    for i, (idx, row) in enumerate(df2[mask_diff].iterrows()):\n",
    "        # Print information about the difference\n",
    "        print(\n",
    "            f\"{row['DENOM_CIA']}, {row['AGRUPAMENTO']}, {row['DT_REFER'].strftime('%Y-%m-%d')}, \"\n",
    "            f\"Old Value: {int(df1.loc[idx, 'VL_CONTA'])} -> New Value: {int(row['VL_CONTA'])}, \"\n",
    "            f\"{row['CD_CONTA']}, {row['DS_CONTA']}\")\n",
    "\n",
    "        # Create filters to identify rows in df2 that match the current difference\n",
    "        filter_mask_cia = df2['DENOM_CIA'] == row['DENOM_CIA']\n",
    "        filter_mask_agg = df2['AGRUPAMENTO'] == row['AGRUPAMENTO']\n",
    "        filter_mask_conta = df2['CD_CONTA'] == row['CD_CONTA']\n",
    "        filter_mask_year = df2['DT_REFER'].dt.year == row['DT_REFER'].year\n",
    "\n",
    "        # Combine filters to create a mask for the matching rows in df2\n",
    "        mask = filter_mask_cia & filter_mask_agg & filter_mask_conta & filter_mask_year\n",
    "\n",
    "        # Append the filtered matching rows to the list\n",
    "        df_diff.append(df2[mask])\n",
    "\n",
    "    # If there are matching rows, concatenate them into a single DataFrame\n",
    "    if df_diff:\n",
    "        df_diff = pd.concat(df_diff, ignore_index=False)\n",
    "\n",
    "    # Return the DataFrame containing differences\n",
    "    return df_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALPARGATAS SA, con, 2014-06-30, Old Value: 3320525 -> New Value: 1000000, 1, Ativo Total\n",
      "ALPARGATAS SA, con, 2015-09-30, Old Value: 3092633 -> New Value: 1000000, 3.01, Receita de Venda de Bens e/ou Serviços\n",
      "ALPARGATAS SA, con, 2016-12-31, Old Value: 340971 -> New Value: 1000000, 6.01, Caixa Líquido Atividades Operacionais\n"
     ]
    }
   ],
   "source": [
    "def update_df_math(df_old, df_new):\n",
    "    df_math = {}  # Initialize an empty dictionary to store DataFrames with differences\n",
    "\n",
    "    # Iterate through the years and DataFrames in df_new\n",
    "    for year, df in df_new.items():\n",
    "        # Check if the corresponding DataFrame exists in df_old and if the year is in the past\n",
    "        if year in df_old and df_old[year].shape == df.shape and year < datetime.datetime.now().year:\n",
    "            \n",
    "            # Call the get_diff function to identify and extract differences\n",
    "            dfs = get_diff(df_old[year], df_new[year])\n",
    "            \n",
    "            # Check if there are differences found\n",
    "            if len(dfs) > 0:\n",
    "                # Store the differences in the dictionary using the year as the key\n",
    "                df_math[year] = dfs\n",
    "    # # Add the current year's DataFrame from df_new to df_math_y\n",
    "    current_year = datetime.datetime.now().year\n",
    "    if current_year in df_new:\n",
    "        df_math[current_year] = df_new[current_year]\n",
    "\n",
    "    return df_math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_cvm_math(demo_cvm, df_math):\n",
    "    for year in demo_cvm.keys():\n",
    "        # Update df_cvm['VL_CONTA'] using the values from df_math['VL_CONTA']\n",
    "        demo_cvm[year].loc[df_math[year].index, 'VL_CONTA'] = df_math[year]['VL_CONTA']\n",
    "    return demo_cvm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entering the smart mathmagic world... It takes long time, came back tomorrow... \n",
      "2014 4 lines, 1 companies, 1 con/682 ('ind', 'con')\n",
      "2014 25.00% 1+3, 0.034123s per item, Remaining: 0h 00m 00s\n",
      "   100.00% 1+0, 0.003118s per item, Remaining: 0h 00m 00s\n",
      "2015 4 lines, 1 companies, 1 con/664 ('ind', 'con')\n",
      "2015 50.00% 2+2, 0.037802s per item, Remaining: 0h 00m 00s\n",
      "   100.00% 1+0, 0.003083s per item, Remaining: 0h 00m 00s\n",
      "2016 4 lines, 1 companies, 1 con/654 ('ind', 'con')\n",
      "2016 75.00% 3+1, 0.038755s per item, Remaining: 0h 00m 00s\n",
      "   100.00% 1+0, 0.003026s per item, Remaining: 0h 00m 00s\n",
      "2023 1,110,501 lines, 704 companies, 254 ind/4 con/704 ('ind', 'con')\n",
      "2023 100.00% 4+0, 0.067627s per item, Remaining: 0h 00m 00s\n",
      "   0.09% 1+1151, 2.050958s per item, Remaining: 0h 39m 20s\n",
      "   0.17% 2+1150, 4.363199s per item, Remaining: 1h 23m 37s\n",
      "   0.26% 3+1149, 4.845485s per item, Remaining: 1h 32m 47s\n",
      "   0.35% 4+1148, 4.880047s per item, Remaining: 1h 33m 22s\n",
      "   0.43% 5+1147, 5.626426s per item, Remaining: 1h 47m 33s\n",
      "   0.52% 6+1146, 6.449075s per item, Remaining: 2h 03m 10s\n",
      "   0.61% 7+1145, 7.039122s per item, Remaining: 2h 14m 19s\n",
      "   0.69% 8+1144, 7.256003s per item, Remaining: 2h 18m 20s\n",
      "   0.78% 9+1143, 7.418147s per item, Remaining: 2h 21m 18s\n",
      "   0.87% 10+1142, 7.690418s per item, Remaining: 2h 26m 22s\n",
      "   0.95% 11+1141, 7.753408s per item, Remaining: 2h 27m 26s\n",
      "   1.04% 12+1140, 7.448687s per item, Remaining: 2h 21m 31s\n",
      "   1.13% 13+1139, 7.181432s per item, Remaining: 2h 16m 19s\n",
      "   1.22% 14+1138, 6.895031s per item, Remaining: 2h 10m 46s\n",
      "   1.30% 15+1137, 6.720683s per item, Remaining: 2h 07m 21s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_math \u001b[39m=\u001b[39m perform_math_magic(df_math)\n",
      "Cell \u001b[1;32mIn[101], line 83\u001b[0m, in \u001b[0;36mperform_math_magic\u001b[1;34m(demo_cvm, max_iterations)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 83\u001b[0m    demonstrativo_cvm\u001b[39m.\u001b[39;49mloc[i2, \u001b[39m'\u001b[39;49m\u001b[39mVL_CONTA\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m=\u001b[39m q2\n\u001b[0;32m     84\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     85\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32md:\\Fausto Stangler\\Documentos\\Python\\DSH\\.venv\\lib\\site-packages\\pandas\\core\\indexing.py:818\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    817\u001b[0m iloc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39miloc\n\u001b[1;32m--> 818\u001b[0m iloc\u001b[39m.\u001b[39;49m_setitem_with_indexer(indexer, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[1;32md:\\Fausto Stangler\\Documentos\\Python\\DSH\\.venv\\lib\\site-packages\\pandas\\core\\indexing.py:1795\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1792\u001b[0m \u001b[39m# align and set the values\u001b[39;00m\n\u001b[0;32m   1793\u001b[0m \u001b[39mif\u001b[39;00m take_split_path:\n\u001b[0;32m   1794\u001b[0m     \u001b[39m# We have to operate column-wise\u001b[39;00m\n\u001b[1;32m-> 1795\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setitem_with_indexer_split_path(indexer, value, name)\n\u001b[0;32m   1796\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1797\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[1;32md:\\Fausto Stangler\\Documentos\\Python\\DSH\\.venv\\lib\\site-packages\\pandas\\core\\indexing.py:1888\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_split_path\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1884\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1885\u001b[0m \n\u001b[0;32m   1886\u001b[0m     \u001b[39m# scalar value\u001b[39;00m\n\u001b[0;32m   1887\u001b[0m     \u001b[39mfor\u001b[39;00m loc \u001b[39min\u001b[39;00m ilocs:\n\u001b[1;32m-> 1888\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setitem_single_column(loc, value, pi)\n",
      "File \u001b[1;32md:\\Fausto Stangler\\Documentos\\Python\\DSH\\.venv\\lib\\site-packages\\pandas\\core\\indexing.py:1992\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_single_column\u001b[1;34m(self, loc, value, plane_indexer)\u001b[0m\n\u001b[0;32m   1988\u001b[0m         value \u001b[39m=\u001b[39m value[pi]\n\u001b[0;32m   1989\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1990\u001b[0m     \u001b[39m# set value into the column (first attempting to operate inplace, then\u001b[39;00m\n\u001b[0;32m   1991\u001b[0m     \u001b[39m#  falling back to casting if necessary)\u001b[39;00m\n\u001b[1;32m-> 1992\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mcolumn_setitem(loc, plane_indexer, value)\n\u001b[0;32m   1993\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_clear_item_cache()\n\u001b[0;32m   1994\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32md:\\Fausto Stangler\\Documentos\\Python\\DSH\\.venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1392\u001b[0m, in \u001b[0;36mBlockManager.column_setitem\u001b[1;34m(self, loc, idx, value, inplace)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1391\u001b[0m     new_mgr \u001b[39m=\u001b[39m col_mgr\u001b[39m.\u001b[39msetitem((idx,), value)\n\u001b[1;32m-> 1392\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miset(loc, new_mgr\u001b[39m.\u001b[39;49m_block\u001b[39m.\u001b[39;49mvalues, inplace\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32md:\\Fausto Stangler\\Documentos\\Python\\DSH\\.venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1219\u001b[0m, in \u001b[0;36mBlockManager.iset\u001b[1;34m(self, loc, value, inplace)\u001b[0m\n\u001b[0;32m   1217\u001b[0m blk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks[blkno]\n\u001b[0;32m   1218\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(blk\u001b[39m.\u001b[39m_mgr_locs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:  \u001b[39m# TODO: fastest way to check this?\u001b[39;00m\n\u001b[1;32m-> 1219\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iset_single(\n\u001b[0;32m   1220\u001b[0m         loc,\n\u001b[0;32m   1221\u001b[0m         value,\n\u001b[0;32m   1222\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[0;32m   1223\u001b[0m         blkno\u001b[39m=\u001b[39;49mblkno,\n\u001b[0;32m   1224\u001b[0m         blk\u001b[39m=\u001b[39;49mblk,\n\u001b[0;32m   1225\u001b[0m     )\n\u001b[0;32m   1227\u001b[0m \u001b[39m# error: Incompatible types in assignment (expression has type\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m \u001b[39m# \"List[Union[int, slice, ndarray]]\", variable has type \"Union[int,\u001b[39;00m\n\u001b[0;32m   1229\u001b[0m \u001b[39m# slice, ndarray]\")\u001b[39;00m\n\u001b[0;32m   1230\u001b[0m loc \u001b[39m=\u001b[39m [loc]  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[1;32md:\\Fausto Stangler\\Documentos\\Python\\DSH\\.venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1358\u001b[0m, in \u001b[0;36mBlockManager._iset_single\u001b[1;34m(self, loc, value, inplace, blkno, blk)\u001b[0m\n\u001b[0;32m   1356\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clear_reference_block(blkno)\n\u001b[0;32m   1357\u001b[0m     iloc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblklocs[loc]\n\u001b[1;32m-> 1358\u001b[0m     blk\u001b[39m.\u001b[39;49mset_inplace(\u001b[39mslice\u001b[39;49m(iloc, iloc \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), value, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[0;32m   1359\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   1361\u001b[0m nb \u001b[39m=\u001b[39m new_block_2d(value, placement\u001b[39m=\u001b[39mblk\u001b[39m.\u001b[39m_mgr_locs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_math = update_df_math(df_old, df_new)\n",
    "df_math = perform_math_magic(df_math)\n",
    "dre_cvm = merge_cvm_math(demo_cvm, df_math)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = {}\n",
    "\n",
    "# Iterating over each year's dataframe in df1\n",
    "for year, df in df1.items():\n",
    "    print(year, len(df))\n",
    "    \n",
    "    # If the year doesn't exist in df2, simply add the dataframe from df1\n",
    "    if year not in df2:\n",
    "        df_merged[year] = df\n",
    "        continue\n",
    "\n",
    "    # Merge on the desired columns. The 'how' parameter ensures we keep all rows from df1 for the given year.\n",
    "    merged_df = df.merge(df2[year], on=['DENOM_CIA', 'AGRUPAMENTO', 'CD_CONTA', 'DT_REFER'], how='left', indicator=True)\n",
    "\n",
    "    # Rows that are in df1 but not in df2 will have the indicator column set to 'left_only'\n",
    "    df_filtered = merged_df[merged_df['_merge'] == 'left_only'][df.columns]\n",
    "    \n",
    "    # Appending df2's current year dataframe to df_filtered\n",
    "    df_merged[year] = pd.concat([df_filtered, df2[year]], ignore_index=True)\n",
    "\n",
    "# Handling years that might be in df2 but not in df1\n",
    "for year in df2:\n",
    "    if year not in df1:\n",
    "        df_merged[year] = df2[year]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll consider all unique years present in both dictionaries\n",
    "all_years = set(df1.keys()) | set(df2.keys())\n",
    "\n",
    "# Iterate over each year\n",
    "for year in all_years:\n",
    "    # If the year is present in both df1 and df2\n",
    "    if year in df1 and year in df2:\n",
    "        # Merge the two dataframes for the given year on all columns\n",
    "        # We use an outer merge to retain all rows and use the indicator column to track the origin\n",
    "        merged_df = df1[year].merge(df2[year], how='outer', indicator=True, on=df1[year].columns.tolist())\n",
    "        \n",
    "        # Filter rows that are only in df1 using the indicator column\n",
    "        unique_df1_rows = merged_df[merged_df['_merge'] == 'left_only'][df1[year].columns.tolist()]\n",
    "        \n",
    "        # Update df2[year] by concatenating the unique rows from df1[year]\n",
    "        df2[year] = pd.concat([df2[year], unique_df1_rows], ignore_index=True)\n",
    "        \n",
    "    # If the year is only present in df1 and not in df2, copy the entire df1[year] to df2\n",
    "    elif year in df1:\n",
    "        df2[year] = df1[year]\n",
    "\n",
    "# No need to handle years that are only in df2 since we're updating df2 directly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# These are the key columns based on which we'll determine if rows are the same between df1 and df2.\n",
    "key_columns = ['DENOM_CIA', 'AGRUPAMENTO', 'CD_CONTA', 'DT_REFER']\n",
    "\n",
    "# Iterate over each year in df1\n",
    "for year in df1:\n",
    "    \n",
    "    # Check if the year exists in df2. If not, simply assign df1's dataframe to df2.\n",
    "    if year not in df2:\n",
    "        df2[year] = df1[year]\n",
    "        continue\n",
    "    \n",
    "    # Merge the dataframes for the current year based on the key columns.\n",
    "    # This is a left merge on df1 with respect to df2. So, all rows from df1[year] will be retained.\n",
    "    # The 'indicator' parameter adds a special column '_merge' to the result which tells us from which dataframe the row came.\n",
    "    merged_df = df1[year].merge(df2[year], on=key_columns, how='left', indicator=True)\n",
    "    \n",
    "    # Filter out rows that are only in df1 (i.e., not in df2).\n",
    "    # These are the rows we need to append to df2[year].\n",
    "    df1_only = merged_df[merged_df['_merge'] == 'left_only']\n",
    "    \n",
    "    # Drop the '_merge' column as it's not needed in the final output.\n",
    "    df1_only = df1_only.drop(columns=['_merge'])\n",
    "    \n",
    "    # Concatenate the rows from df1 that are not in df2 to df2[year].\n",
    "    # This updates df2[year] to include rows from both dataframes, but without any duplicates based on our key columns.\n",
    "    df2[year] = pd.concat([df2[year], df1_only], ignore_index=True)\n",
    "\n",
    "# At the end of this loop, df2 will have been updated to include rows from both df1 and df2 for each year, \n",
    "# without any duplicate rows based on the key columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The key columns used to determine unique rows\n",
    "key_columns = ['DENOM_CIA', 'AGRUPAMENTO', 'CD_CONTA', 'DT_REFER']\n",
    "\n",
    "# Create an empty dictionary to store the merged dataframes for each year\n",
    "df_merged = {}\n",
    "\n",
    "# Iterate over each year present in either df1 or df2\n",
    "for year in set(df1.keys()).union(df2.keys()):\n",
    "    # Check if the year exists in df2 (the newer dataframe)\n",
    "    if year in df2:\n",
    "        # Start with the data from df2 for this year\n",
    "        merged_df = df2[year].copy()\n",
    "        \n",
    "        # Check if the year also exists in df1 (the older dataframe)\n",
    "        if year in df1:\n",
    "            # Determine the rows in df1[year] that are not present in df2[year]\n",
    "            # based on the key columns\n",
    "            mask = ~df1[year].set_index(key_columns).index.isin(df2[year].set_index(key_columns).index)\n",
    "            \n",
    "            # Filter df1[year] to only these rows\n",
    "            extra_rows = df1[year][mask]\n",
    "            \n",
    "            # Append these rows to the merged dataframe\n",
    "            merged_df = merged_df.append(extra_rows)\n",
    "    else:\n",
    "        # If the year doesn't exist in df2, then just use the data from df1\n",
    "        merged_df = df1[year].copy()\n",
    "\n",
    "    # Store the merged dataframe for this year in the df_merged dictionary\n",
    "    df_merged[year] = merged_df\n",
    "\n",
    "# At this point, df_merged will have the merged data for each year\n",
    "print(df_merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download clean and organize databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cvm = 'https://dados.cvm.gov.br/dados/CIA_ABERTA/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_pre = load_pkl(f'companies_pre')\n",
    "companies_pre['ALPARGATAS SA']\n",
    "df = companies_pre['ALPARGATAS SA']\n",
    "\n",
    "mask_CD_CONTA = df['CD_CONTA'] == '6.01'\n",
    "mask_AGRUPAMENTO = df['AGRUPAMENTO'] == 'con'\n",
    "mask_DT_INI_EXERC = df['DT_INI_EXERC'].dt.month == 1\n",
    "mask = mask_CD_CONTA & mask_AGRUPAMENTO & mask_DT_INI_EXERC\n",
    "\n",
    "df[mask][['DT_REFER', 'VL_CONTA']].set_index('DT_REFER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = load_pkl(f'companies')\n",
    "companies['ALPARGATAS SA']\n",
    "df = companies['ALPARGATAS SA']\n",
    "\n",
    "mask_CD_CONTA = df['CD_CONTA'] == '6.01'\n",
    "mask_AGRUPAMENTO = df['AGRUPAMENTO'] == 'con'\n",
    "mask_DT_INI_EXERC = df['DT_INI_EXERC'].dt.month == 1\n",
    "mask = mask_CD_CONTA & mask_AGRUPAMENTO & mask_DT_INI_EXERC\n",
    "\n",
    "df[mask][['DT_REFER', 'VL_CONTA']].set_index('DT_REFER').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_cvm[2019].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mathmagic(df):\n",
    "    df['DT_REFER'] = pd.to_datetime(df['DT_REFER'])\n",
    "    try:\n",
    "        i1 = df[df['DT_REFER'].dt.quarter == 1].index[0]\n",
    "        q1 = df[df['DT_REFER'].dt.quarter == 1]['VL_CONTA'].iloc[0]\n",
    "    except Exception as e:\n",
    "        q1 = 0\n",
    "    try:\n",
    "        i2 = df[df['DT_REFER'].dt.quarter == 2].index[0]\n",
    "        q2 = df[df['DT_REFER'].dt.quarter == 2]['VL_CONTA'].iloc[0]\n",
    "    except Exception as e:\n",
    "        q2 = 0\n",
    "    try:\n",
    "        i3 = df[df['DT_REFER'].dt.quarter == 3].index[0]\n",
    "        q3 = df[df['DT_REFER'].dt.quarter == 4]['VL_CONTA'].iloc[0]\n",
    "    except Exception as e:\n",
    "        q3 = 0\n",
    "    try:\n",
    "        i4 = df[df['DT_REFER'].dt.quarter == 4].index[0]\n",
    "        q4 = df[df['DT_REFER'].dt.quarter == 4]['VL_CONTA'].iloc[0]\n",
    "    except Exception as e:\n",
    "        q4 = 0\n",
    "\n",
    "\n",
    "        # try:\n",
    "        #     # Perform calculations based on quarters and update flag\n",
    "        #     if conta_first in last_quarters:\n",
    "        #         if not demonstrativo_cvm.loc[i4, 'MATH_MAGIC']:\n",
    "        #             q4 = q4 - (q3 + q2 + q1)\n",
    "        #         update = True\n",
    "        #     elif conta_first in all_quarters:\n",
    "        #         if not demonstrativo_cvm.loc[i2, 'MATH_MAGIC']:\n",
    "        #             q2 = q2 - (q1)\n",
    "        #         if not demonstrativo_cvm.loc[i3, 'MATH_MAGIC']:\n",
    "        #             q3 = q3 - (q2 + q1)\n",
    "        #         if not demonstrativo_cvm.loc[i4, 'MATH_MAGIC']:\n",
    "        #             q4 = q4 - (q3 + q2 + q1)\n",
    "        #         update = True\n",
    "        # except Exception as e:\n",
    "        #     update = False\n",
    "\n",
    "        # # Update demo_cvm data based on calculations\n",
    "        # if update:\n",
    "        #     demonstrativo_cvm.loc[i2, ['VL_CONTA', 'MATH_MAGIC']] = [q2, True]\n",
    "        #     demonstrativo_cvm.loc[i3, ['VL_CONTA', 'MATH_MAGIC']] = [q3, True]\n",
    "        #     demonstrativo_cvm.loc[i4, ['VL_CONTA', 'MATH_MAGIC']] = [q4, True]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in companies_by_str_port.items():\n",
    "    print(k)\n",
    "    print(type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def perform_math_magic(demo_cvm, last_quarters, all_quarters, max_iterations=20000000):\n",
    "    \"\"\"\n",
    "    Perform 'magic' calculations on the DataFrame demo_cvm based on specified quarters.\n",
    "\n",
    "    Args:\n",
    "        demo_cvm (dict): Dictionary of DataFrames containing financial data.\n",
    "        last_quarters (list): List of quarters considered as last quarters.\n",
    "        all_quarters (list): List of quarters considered as all quarters.\n",
    "        max_iterations (int): Maximum number of iterations to perform.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated demo_cvm with 'magic' calculations.\n",
    "\n",
    "    This function iterates through the provided demo_cvm DataFrames, performs calculations based on specified quarters,\n",
    "    and updates the 'VL_CONTA' values and 'MATH_MAGIC' flag where necessary.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate through each year's DataFrame\n",
    "    for n1, (year, demonstrativo_cvm) in enumerate(demo_cvm.items()):\n",
    "        companies_by_str_port = get_companies_by_str_port(demonstrativo_cvm)\n",
    "        groups = demonstrativo_cvm.groupby(['DENOM_CIA', 'AGRUPAMENTO'], group_keys=False)\n",
    "        \n",
    "        start_time_2 = time.time()\n",
    "        for n2, (key, group) in enumerate(groups):\n",
    "            company = key[0]\n",
    "            agg = key[1]\n",
    "            subgroups = group.groupby(['CD_CONTA', 'DS_CONTA'], group_keys=False)\n",
    "            \n",
    "            start_time_3 = time.time()\n",
    "            for n3, (index, df) in enumerate(subgroups):\n",
    "                conta_first = index[0][0]\n",
    "                # Convert DT_REFER to datetime\n",
    "                df['DT_REFER'] = pd.to_datetime(df['DT_REFER'])\n",
    "                \n",
    "                # Create 'MATH_MAGIC' column if not exists\n",
    "                if 'MATH_MAGIC' not in demonstrativo_cvm.columns:\n",
    "                    demonstrativo_cvm['MATH_MAGIC'] = False\n",
    "\n",
    "                try:\n",
    "                    i1 = df[df['DT_REFER'].dt.quarter == 1].index[0]\n",
    "                    q1 = df[df['DT_REFER'].dt.quarter == 1]['VL_CONTA'].iloc[0]\n",
    "                except Exception:\n",
    "                    q1 = 0\n",
    "                try:\n",
    "                    i2 = df[df['DT_REFER'].dt.quarter == 2].index[0]\n",
    "                    q2 = df[df['DT_REFER'].dt.quarter == 2]['VL_CONTA'].iloc[0]\n",
    "                except Exception:\n",
    "                    q2 = 0\n",
    "                try:\n",
    "                    i3 = df[df['DT_REFER'].dt.quarter == 3].index[0]\n",
    "                    q3 = df[df['DT_REFER'].dt.quarter == 4]['VL_CONTA'].iloc[0]\n",
    "                except Exception:\n",
    "                    q3 = 0\n",
    "                try:\n",
    "                    i4 = df[df['DT_REFER'].dt.quarter == 4].index[0]\n",
    "                    q4 = df[df['DT_REFER'].dt.quarter == 4]['VL_CONTA'].iloc[0]\n",
    "                except Exception:\n",
    "                    q4 = 0\n",
    "\n",
    "                update = False\n",
    "                try:\n",
    "                    # Perform calculations based on specified quarters and update flag\n",
    "                    if conta_first in last_quarters:\n",
    "                        if not demonstrativo_cvm.loc[i4, 'MATH_MAGIC']:\n",
    "                            q4 = q4 - (q3 + q2 + q1)\n",
    "                        update = True\n",
    "                    elif conta_first in all_quarters:\n",
    "                        if not demonstrativo_cvm.loc[i2, 'MATH_MAGIC']:\n",
    "                            q2 = q2 - (q1)\n",
    "                        if not demonstrativo_cvm.loc[i3, 'MATH_MAGIC']:\n",
    "                            q3 = q3 - (q2 + q1)\n",
    "                        if not demonstrativo_cvm.loc[i4, 'MATH_MAGIC']:\n",
    "                            q4 = q4 - (q3 + q2 + q1)\n",
    "                        update = True\n",
    "                except Exception:\n",
    "                    update = False\n",
    "\n",
    "                if update:\n",
    "                    # Update 'VL_CONTA' values and 'MATH_MAGIC' flag\n",
    "                    demonstrativo_cvm.loc[i2, ['VL_CONTA', 'MATH_MAGIC']] = [q2, True]\n",
    "                    demonstrativo_cvm.loc[i3, ['VL_CONTA', 'MATH_MAGIC']] = [q3, True]\n",
    "                    demonstrativo_cvm.loc[i4, ['VL_CONTA', 'MATH_MAGIC']] = [q4, True]\n",
    "\n",
    "                if n3 > max_iterations:\n",
    "                    break\n",
    "            if n2 > max_iterations:\n",
    "                break\n",
    "        if n1 > max_iterations:\n",
    "            break\n",
    "\n",
    "    return demo_cvm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i1, i2, i3, i4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conta_first in ['1']:\n",
    "    print('sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstrativo_cvm.loc[i4, 'MATH_MAGIC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstrativo_cvm.loc[[i1, i2, i3, i4]]['VL_CONTA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['VL_CONTA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def by_year(demo_cvm):\n",
    "    \"\"\"\n",
    "    Process demo_cvm data by performing calculations and updates on a per-year basis.\n",
    "\n",
    "    Args:\n",
    "        demo_cvm (dict): Dictionary containing demo_cvm data for different years.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated demo_cvm data after processing.\n",
    "\n",
    "    This function iterates through each year's demo_cvm data, performs calculations,\n",
    "    and updates the data based on specific conditions. It handles quarter-wise calculations\n",
    "    and adjustments for various financial data, such as account balances and quarters.\n",
    "\n",
    "    Args:\n",
    "        demo_cvm (dict): A dictionary containing demo_cvm data for different years.\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated demo_cvm data after applying calculations and adjustments.\n",
    "\n",
    "    Loop Explanation:\n",
    "    - The function iterates over each year's demo_cvm data to process the financial data.\n",
    "    - Inside the year loop, the function groups data by companies and structured reports (ind, con).\n",
    "    - For each group, it calculates the total number of lines and unique companies involved.\n",
    "    - It then iterates over subgroups, each representing a company and a specific account.\n",
    "    - For each subgroup, the function performs calculations based on quarters and updates\n",
    "      the data accordingly, ensuring correct financial data representation.\n",
    "\n",
    "    The `n` parameter is used as a limit for iterations during development, providing faster\n",
    "    testing and debugging. Adjust the value of `n` to match your needs for data processing.\n",
    "\n",
    "    Example:\n",
    "        To use this function, pass the `demo_cvm` dictionary containing financial data by year.\n",
    "        The function will iterate through the years and process financial data accordingly,\n",
    "        ensuring accurate calculations and adjustments.\n",
    "\n",
    "    \"\"\"\n",
    "    # Define relevant quarters\n",
    "    last_quarters = ['3', '4']\n",
    "    all_quarters = ['6', '7']\n",
    "\n",
    "    # Large number for limiting iterations during development\n",
    "    n = 1000000000\n",
    "\n",
    "    # Start time for overall execution\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Loop through each year's data in demo_cvm\n",
    "    for n1, (year, demonstrativo_cvm) in enumerate(demo_cvm.items()):\n",
    "\n",
    "        # Get companies by structured report (ind, con)\n",
    "        companies_by_str_port = get_companies_by_str_port(demonstrativo_cvm)\n",
    "        print(f\"{year} {len(demonstrativo_cvm):,.0f} lines, {len(demonstrativo_cvm['DENOM_CIA'].unique())} companies, {'/'.join([f'{len(companies)} {key}' for key, companies in companies_by_str_port.items()])}\")\n",
    "        print(year, remaining_time(start_time, len(demo_cvm), n1))\n",
    "\n",
    "        # Group by 'DENOM_CIA' and 'AGRUPAMENTO'\n",
    "        groups = demonstrativo_cvm.groupby(['DENOM_CIA', 'AGRUPAMENTO'], group_keys=False)\n",
    "\n",
    "        # Start time for grouping by company and account\n",
    "        start_time_2 = time.time()\n",
    "\n",
    "        # Iterate through groups (company and account)\n",
    "        for n2, (key, group) in enumerate(groups):\n",
    "            print('  ', remaining_time(start_time_2, len(groups), n2))\n",
    "            company = key[0]\n",
    "            agg = key[1]\n",
    "            \n",
    "            # Group by 'CD_CONTA' and 'DS_CONTA'\n",
    "            subgroups = group.groupby(['CD_CONTA', 'DS_CONTA'], group_keys=False)\n",
    "\n",
    "            # Start time for processing subgroups\n",
    "            start_time_3 = time.time()\n",
    "\n",
    "            # Iterate through subgroups (account)\n",
    "            for n3, (index, df) in enumerate(subgroups):\n",
    "                # print('  ', '  ', remaining_time(start_time_3, len(subgroups), n3))\n",
    "                conta = index[0]\n",
    "                conta_first = index[0][0]\n",
    "                descricao = index[1]\n",
    "\n",
    "                # Update flag for whether to perform calculations\n",
    "                update = False\n",
    "\n",
    "                # Check if all quarters are present in 'DT_REFER' column\n",
    "                if all(q in df['DT_REFER'].dt.quarter.values for q in [1, 2, 3, 4]):\n",
    "                    i1 = df[df['DT_REFER'].dt.quarter == 1].index[0]\n",
    "                    i2 = df[df['DT_REFER'].dt.quarter == 2].index[0]\n",
    "                    i3 = df[df['DT_REFER'].dt.quarter == 3].index[0]\n",
    "                    i4 = df[df['DT_REFER'].dt.quarter == 4].index[0]\n",
    "\n",
    "                    q1 = df[df['DT_REFER'].dt.quarter == 1]['VL_CONTA'].iloc[0]\n",
    "                    q2 = df[df['DT_REFER'].dt.quarter == 2]['VL_CONTA'].iloc[0]\n",
    "                    q3 = df[df['DT_REFER'].dt.quarter == 3]['VL_CONTA'].iloc[0]\n",
    "                    q4 = df[df['DT_REFER'].dt.quarter == 4]['VL_CONTA'].iloc[0]\n",
    "                \n",
    "                    try:\n",
    "                        # Perform calculations based on quarters and update flag\n",
    "                        if conta_first in last_quarters:\n",
    "                            if not demonstrativo_cvm.loc[i4, 'MATH_MAGIC']:\n",
    "                                q4 = q4 - (q3 + q2 + q1)\n",
    "                            update = True\n",
    "                        elif conta_first in all_quarters:\n",
    "                            if not demonstrativo_cvm.loc[i2, 'MATH_MAGIC']:\n",
    "                                q2 = q2 - (q1)\n",
    "                            if not demonstrativo_cvm.loc[i3, 'MATH_MAGIC']:\n",
    "                                q3 = q3 - (q2 + q1)\n",
    "                            if not demonstrativo_cvm.loc[i4, 'MATH_MAGIC']:\n",
    "                                q4 = q4 - (q3 + q2 + q1)\n",
    "                            update = True\n",
    "                    except Exception as e:\n",
    "                        update = False\n",
    "\n",
    "                    # Update demo_cvm data based on calculations\n",
    "                    if update:\n",
    "                        demonstrativo_cvm.loc[i2, ['VL_CONTA', 'MATH_MAGIC']] = [q2, True]\n",
    "                        demonstrativo_cvm.loc[i3, ['VL_CONTA', 'MATH_MAGIC']] = [q3, True]\n",
    "                        demonstrativo_cvm.loc[i4, ['VL_CONTA', 'MATH_MAGIC']] = [q4, True]\n",
    "\n",
    "                if n3 > n:\n",
    "                    break\n",
    "            if n2 > n:\n",
    "                break\n",
    "        if n1 > n:\n",
    "            break\n",
    "    return demo_cvm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f'dataframe_{k_year}'\n",
    "demo_cvm[k_year] = save_pkl(demo_cvm[k_year], file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_cvm = load_pkl('dataframes')\n",
    "years = [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023]\n",
    "for k_year in years:\n",
    "    for y in [2010, 1011, 2012, 2013, 2014]:\n",
    "        try:\n",
    "            del demo_cvm[y]\n",
    "        except:\n",
    "            pass\n",
    "    print(k_year)\n",
    "    keys_to_delete = []\n",
    "    for year, df in demo_cvm.items():\n",
    "        if year != k_year:\n",
    "            keys_to_delete.append(year)\n",
    "    for key in keys_to_delete:\n",
    "        del demo_cvm[key]\n",
    "    demo_cvm = by_year(demo_cvm)\n",
    "    \n",
    "    file_name = f'dataframe_{k_year}'\n",
    "    demo_cvm[k_year] = save_pkl(demo_cvm[k_year], file_name)\n",
    "    demo_cvm = load_pkl('dataframes')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2010, 2024)\n",
    "demo_cvm = {}\n",
    "start_time = time.time()\n",
    "for i, year in enumerate(years):\n",
    "    print(year)\n",
    "    demo_cvm[year] = load_pkl(f'dataframe_{year}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save by company\n",
    "## get an aggregated list of company, agg in all demo_cvm dict\n",
    "## create a demo_cvm_company dict, transform from year to company\n",
    "\n",
    "## must create a setor, subsetor, segmento for companies, then use as keys?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique companies across all years\n",
    "all_companies = set()\n",
    "for df in years:\n",
    "    all_companies.update(df['company-agg'].unique())\n",
    "\n",
    "# Initialize the final dictionary with companies as keys\n",
    "company_dict = {}\n",
    "\n",
    "# Populate the company_dict\n",
    "for company in all_companies:\n",
    "    company_data = []  # This will hold dataframes for each year for the company\n",
    "    for year, df in year_dict.items():\n",
    "        company_df_for_year = df[df['company-agg'] == company]\n",
    "        company_data.append(company_df_for_year)\n",
    "    \n",
    "    # Concatenate the data for the company across all years\n",
    "    company_dict[company] = pd.concat(company_data, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = demo_cvm[2014]\n",
    "\n",
    "m_company = df['DENOM_CIA'] == 'CENTRAIS ELET BRAS SA  ELETROBRAS'\n",
    "m_conta = df['BALANCE_SHEET'] == 'DRE'\n",
    "m_conta_len = df['CD_CONTA'].str.len() <= 4\n",
    "m_agg = df['AGRUPAMENTO'] == 'ind'\n",
    "mask = m_company & m_conta & m_conta_len & m_agg\n",
    "\n",
    "# Filter the DataFrame using the mask\n",
    "filtered_df = df[mask][['DS_CONTA', 'DT_REFER', 'VL_CONTA']]\n",
    "\n",
    "# Pivot the filtered DataFrame using pivot_table and aggregation function\n",
    "pivot_df = filtered_df.pivot_table(index='DT_REFER', columns='DS_CONTA', values='VL_CONTA', aggfunc='sum')\n",
    "\n",
    "# Plot the pivoted DataFrame\n",
    "try:\n",
    "    pivot_df.plot()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CD_CONTA'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_company = df['DENOM_CIA'] == 'ALPARGATAS SA'\n",
    "m_conta = df['BALANCE_SHEET'] == 'DRE'\n",
    "m_conta_len = df['CD_CONTA'].str.len() <= 4\n",
    "m_agg = df['AGRUPAMENTO'] == 'ind'\n",
    "mask = m_company & m_conta & m_conta_len & m_agg\n",
    "df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o que caracteriza uma df completa, um relatório estruturado? DENOM_CIA + DT_REFER\n",
    "# como saber quais empresas são 'con' e quais são 'ind' e como criar uma terceira view mae = con - ind?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_by_relest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SÓ INDIVIDUAL ind = True, con = False: '3A COMPANHIA SECURITIZADORA'\n",
    "# SÓ CONSOLIDADO ind = False, con = True: 'BANCO SANTANDER SA'\n",
    "# AMBOS ind = True, con = True: 'ADVANCED DIGITAL HEALTH MEDICINA PREVENTIVA SA'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_by_relest['individual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in companies_by_relest.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relest_individual = True\n",
    "relest_consolidado = False\n",
    "pivot_table[(pivot_table['ind'] == relest_individual) & (pivot_table['con'] == relest_consolidado)].index.get_level_values('DENOM_CIA').unique()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cia_cols = ['CNPJ_CIA', 'DENOM_CIA', 'CD_CVM'] # 683 rows\n",
    "bal_cols = ['FILE_NAME', 'demonstrativo_cvm', 'BALANCE_SHEET', 'AGRUPAMENTO', 'GRUPO_DFP'] # 32 rows\n",
    "dt_cols = ['DT_REFER', 'DT_FIM_EXERC'] # 4 rows\n",
    "cod_conta_cols = ['CD_CONTA', ] # 1543 rows\n",
    "desc_conta_cols = ['DS_CONTA', ] # 38597 rows\n",
    "vlr_cta_cols = ['VL_CONTA',]\n",
    "\n",
    "ubiq_cols = ['ANO', 'MOEDA'] # 1 row\n",
    "unique__independ_cols = ['VERSAO', 'ESCALA_MOEDA',  'ST_CONTA_FIXA', 'DT_INI_EXERC', 'COLUNA_DF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df[cia_cols+bal_cols+dt_cols+cod_conta_cols+desc_conta_cols+vlr_cta_cols+unique__independ_cols]\n",
    "mask = df_temp['CD_CVM'] == 2437\n",
    "mask2 = df_temp['AGRUPAMENTO'] == 'con'\n",
    "mask3 = df_temp['BALANCE_SHEET'] == 'DRE'\n",
    "mask4 = df_temp['DT_REFER'].dt.month == 12\n",
    "mask5 = df_temp['CD_CONTA'] == '3.01'\n",
    "\n",
    "df_temp[mask&mask2&mask3&mask5].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by year\n",
    "\n",
    "# clean df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.groupby('CD_CVM')['VERSAO'].idxmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_demo_cvm_y[year]\n",
    "if 1 == 1:\n",
    "    df = df.copy()  # Create a copy of the DataFrame\n",
    "\n",
    "    # filter only last one\n",
    "    df = df[df['ORDEM_EXERC'] == 'ÚLTIMO']\n",
    "    df = df.drop(columns=['ORDEM_EXERC'])\n",
    "\n",
    "    # Clean up Text\n",
    "    df['DENOM_CIA'] = df['DENOM_CIA'].apply(clean_text)\n",
    "\n",
    "    # rows to remove\n",
    "    words_to_remove = ['LIQUIDACAO', 'JUDICIAL', ]\n",
    "    df = df[~df['DENOM_CIA'].str.contains('|'.join(words_to_remove))]\n",
    "\n",
    "    # DateTime\n",
    "    date_columns = ['DT_REFER', 'DT_RECEB', 'DT_FIM_EXERC', ]\n",
    "    # Convert the specified date columns to datetime\n",
    "    for col in date_columns:\n",
    "        try:\n",
    "            df = df.assign(**{col: pd.to_datetime(df[col])})\n",
    "        except Exception as e:\n",
    "            # Handle invalid date values here\n",
    "            pass\n",
    "\n",
    "    # Sort the DataFrame by 'CD_CVM' and 'VERSAO' in descending order\n",
    "    sorted_df = df.sort_values(by=['CD_CVM', 'VERSAO'], ascending=[True, False])\n",
    "\n",
    "    # After sorting\n",
    "    sorted_df = df.sort_values(by=['CD_CVM', 'VERSAO'], ascending=[True, False])\n",
    "    print(\"After sorting:\", sorted_df.shape)\n",
    "\n",
    "    # After the groupby and idxmax operations\n",
    "    indices_of_max_version = sorted_df.groupby('CD_CVM')['VERSAO'].idxmax()\n",
    "    print(\"Indices of max version:\", indices_of_max_version)\n",
    "\n",
    "\n",
    "    # Get the indices of the rows with the highest version for each 'CD_CVM'\n",
    "    indices_of_max_version = sorted_df.groupby('CD_CVM')['VERSAO'].idxmax()\n",
    "\n",
    "    # Filter the DataFrame to keep only the rows with the highest version for each 'CD_CVM'\n",
    "    filtered_df = sorted_df.loc[indices_of_max_version]\n",
    "\n",
    "    df = sorted_df = df.sort_values(by=['DENOM_CIA'], ascending=[True])\n",
    "\n",
    "\n",
    "    # return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo_cvm_y = group_by_year(df_demo_cvm, df_demo_cvm_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo_cvm_y[year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo_cvm_links_y[year][df_demo_cvm_links_y[year]['CD_CVM'] == 15253].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df.merge(df_demo_cvm_links[df_demo_cvm_links['ANO'] == '2023'][cols_common + cols_to_add], on=cols_common, how='left')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[cols_common].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo_cvm_links[cols_common].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "\n",
    "dfp.to_csv('dfp.csv')\n",
    "dfp_links.to_csv('dfp_links.csv')\n",
    "itr.to_csv('itr.csv')\n",
    "itr_links.to_csv('itr_links.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from csv\n",
    "\n",
    "dfp = pd.read_csv('dfp.csv')\n",
    "dfp_links = pd.read_csv('dfp_links.csv')\n",
    "itr = pd.read_csv('itr.csv')\n",
    "itr_links = pd.read_csv('itr_links.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL\n",
    "url_raw =        'https://www.rad.cvm.gov.br/ENET/frmGerenciaPaginaFRE.aspx?CodigoTipoInstituicao=1&NumeroSequencialDocumento={ID_DOC}'\n",
    "url_download = 'http://www.rad.cvm.gov.br/ENETCONSULTA/frmDownloadDocumento.aspx?CodigoInstituicao=1&NumeroSequencialDocumento={ID_DOC}'\n",
    "url_relatorio_administracao = 'https://www.rad.cvm.gov.br/ENET/frmExibirArquivoFRE.aspx?NumeroSequencialDocumento=8299&CodigoGrupo=1653&CodigoQuadro=0&Tipo=PDF&RelatorioRevisaoEspecial=Sem+Ressalva&CodTipoDocumento=4'\n",
    "\n",
    "# Generate list of URLs\n",
    "dfp_url_list = [url_raw.format(ID_DOC=id_doc) for id_doc in dfp_links['ID_DOC']]\n",
    "dfp_download_list = [url_download.format(ID_DOC=id_doc) for id_doc in dfp_links['ID_DOC']]\n",
    "dfp_relatorio_administracao_list = [url_relatorio_administracao.format(ID_DOC=id_doc) for id_doc in dfp_links['ID_DOC']]\n",
    "itr_url_list = [url_raw.format(ID_DOC=id_doc) for id_doc in itr_links['ID_DOC']]\n",
    "itr_download_list = [url_download.format(ID_DOC=id_doc) for id_doc in itr_links['ID_DOC']]\n",
    "itr_relatorio_administracao_list = [url_relatorio_administracao.format(ID_DOC=id_doc) for id_doc in itr_links['ID_DOC']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_version_old(df, links):\n",
    "    cols_to_add = ['VERSAO_DOC', 'CATEG_DOC', 'ID_DOC', 'DT_RECEB']\n",
    "    \n",
    "    # Create the columns if they don't exist\n",
    "    for col in cols_to_add:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    \n",
    "    # Group the df DataFrame by the conditions\n",
    "    grouped_df = df.groupby(['ANO', 'demonstrativo_cvm', 'CNPJ_CIA', 'CD_CVM', 'DT_REFER'])\n",
    "    \n",
    "    # Iterate through links and update grouped_df\n",
    "    for i, row in links.iterrows():\n",
    "        key = (row['ANO'], row['demonstrativo_cvm'], row['CNPJ_CIA'], row['CD_CVM'], row['DT_REFER'])\n",
    "        group = grouped_df.get_group(key)\n",
    "        \n",
    "        cols_to_update = ['VERSAO', 'CATEG_DOC', 'ID_DOC', 'DT_RECEB']\n",
    "        df.loc[group.index, cols_to_add] = row[cols_to_update].values\n",
    "        \n",
    "        print(f\"{i+1}/{len(links)-i} {row['demonstrativo_cvm']} {row['ANO']} - {row['CD_CVM']} {row['CNPJ_CIA']} - {row['DT_REFER']} versão {row['VERSAO']}\")\n",
    "    \n",
    "    while i > 20:\n",
    "        print('break')\n",
    "        break\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_version(df, links, grouped_df):\n",
    "    cols_to_add = ['VERSAO_DOC', 'CATEG_DOC', 'ID_DOC', 'DT_RECEB']\n",
    "\n",
    "    # Create the columns if they don't exist\n",
    "    for col in cols_to_add:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    # Convert columns to object type\n",
    "    for col in cols_to_add:\n",
    "        try:\n",
    "            df[col] = df[col].astype('object')\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    for i, key in enumerate(grouped_df.groups.keys()):\n",
    "        print(f'{i}/{len(grouped_df)} {key}')\n",
    "        \n",
    "        grouped_df.get_group(key)\n",
    "        ano, demonstrativo_cvm, cnpj_cia, cd_cvm, dt_refer = key\n",
    "\n",
    "        # Filter links based on the group's conditions\n",
    "        mask = (\n",
    "            (links['ANO'] == ano) &\n",
    "            (links['demonstrativo_cvm'] == demonstrativo_cvm) &\n",
    "            (links['CNPJ_CIA'] == cnpj_cia) &\n",
    "            (links['CD_CVM'] == cd_cvm) &\n",
    "            (links['DT_REFER'] == dt_refer)\n",
    "        )\n",
    "\n",
    "\n",
    "        df.loc[grouped_df.get_group(key).index, cols_to_add] = links[mask][cols_to_add].values\n",
    "\n",
    "    # Convert columns to object type\n",
    "    for col in cols_to_add:\n",
    "        try:\n",
    "            df[col] = df[col].astype('category')\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the df DataFrame by the conditions\n",
    "group_columns = ['ANO', 'demonstrativo_cvm', 'CNPJ_CIA', 'CD_CVM', 'DT_REFER']\n",
    "grouped_df = itr.groupby(group_columns)\n",
    "\n",
    "itr = update_version(itr, itr_links, grouped_df)\n",
    "\n",
    "\n",
    "# # Group the df DataFrame by the conditions\n",
    "# group_columns = ['ANO', 'demonstrativo_cvm', 'CNPJ_CIA', 'CD_CVM', 'DT_REFER']\n",
    "# grouped_df = dfp.groupby(group_columns)\n",
    "\n",
    "# dfp = update_version(dfp, dfp_links, grouped_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
